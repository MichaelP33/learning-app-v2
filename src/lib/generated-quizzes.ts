import type { Quiz } from "@/types";

// AUTO-GENERATED by scripts/generate-quizzes-registry.mjs
export const externalQuizzes: Record<string, Quiz> = {
  "agile-scrum": {
  "title": "Agile/Scrum Knowledge Quiz",
  "totalQuestions": 10,
  "totalPoints": 24,
  "questions": [
    {
      "id": "1",
      "type": "multiple-choice",
      "points": 2,
      "question": "Primary Scrum artifact tying scope to delivery?",
      "options": [
        "Product Backlog",
        "Burnup Chart",
        "Definition of Done",
        "Sprint Velocity"
      ],
      "correctAnswer": 0,
      "additionalContext": "The Product Backlog is the ordered list of everything needed in the product; prioritization ties scope to delivery sequence.",
      "keyConcepts": [
        "Product Backlog",
        "Prioritization",
        "Scope"
      ]
    },
    {
      "id": "2",
      "type": "multiple-choice",
      "points": 2,
      "question": "Timebox for a standard Scrum sprint?",
      "options": [
        "1-2 days",
        "1-4 weeks",
        "6-8 weeks",
        "Quarterly"
      ],
      "correctAnswer": 1,
      "additionalContext": "Scrum sprints are short, consistent timeboxes (often 2 weeks) to enable frequent inspection and adaptation.",
      "keyConcepts": [
        "Timebox",
        "Sprint"
      ]
    },
    {
      "id": "3",
      "type": "multiple-choice",
      "points": 2,
      "question": "Who prioritizes the Product Backlog?",
      "options": [
        "Scrum Master",
        "Product Owner",
        "Developers",
        "Project Manager"
      ],
      "correctAnswer": 1,
      "additionalContext": "The Product Owner maximizes product value by ordering the backlog according to outcomes and risk.",
      "keyConcepts": [
        "Product Owner",
        "Backlog Ordering"
      ]
    },
    {
      "id": "4",
      "type": "multiple-choice",
      "points": 2,
      "question": "Daily Scrum goal?",
      "options": [
        "Approve budget",
        "Plan next increment",
        "Inspect progress and adapt plan",
        "Demo to stakeholders"
      ],
      "correctAnswer": 2,
      "additionalContext": "The team inspects progress toward the Sprint Goal and adapts the plan for the next 24 hours.",
      "keyConcepts": [
        "Sprint Goal",
        "Inspect & Adapt"
      ]
    },
    {
      "id": "5",
      "type": "multiple-choice",
      "points": 2,
      "question": "Definition of Done ensures:",
      "options": [
        "Scope flexibility",
        "Release cadence",
        "Quality and completeness",
        "Executive approval"
      ],
      "correctAnswer": 2,
      "additionalContext": "DoD is the quality bar for increments; work meeting DoD is potentially releasable.",
      "keyConcepts": [
        "Definition of Done",
        "Quality Gates"
      ]
    },
    {
      "id": "6",
      "type": "multiple-choice",
      "points": 2,
      "question": "Sprint Review focuses on:",
      "options": [
        "Team grievances",
        "Hiring decisions",
        "Increment inspection with stakeholders",
        "Quarterly roadmap"
      ],
      "correctAnswer": 2,
      "additionalContext": "The team and stakeholders inspect the increment and adapt the Product Backlog.",
      "keyConcepts": [
        "Increment",
        "Stakeholders"
      ]
    },
    {
      "id": "7",
      "type": "multiple-choice",
      "points": 2,
      "question": "Sprint Retrospective outcome:",
      "options": [
        "Shippable increment",
        "Updated Definition of Ready",
        "Improvement actions",
        "Budget approval"
      ],
      "correctAnswer": 2,
      "additionalContext": "Retro produces concrete improvement actions to enhance process, tools, or collaboration.",
      "keyConcepts": [
        "Retrospective",
        "Continuous Improvement"
      ]
    },
    {
      "id": "8",
      "type": "multiple-choice",
      "points": 2,
      "question": "Velocity best practice:",
      "options": [
        "Compare teams",
        "Treat as target",
        "Use for team forecasting",
        "Publish to executives"
      ],
      "correctAnswer": 2,
      "additionalContext": "Velocity is for team forecasting; it should not be used to compare teams or set quotas.",
      "keyConcepts": [
        "Velocity",
        "Forecasting"
      ]
    },
    {
      "id": "9",
      "type": "freeform",
      "points": 4,
      "question": "Describe one concrete change you would propose after a retrospective and how you would measure its impact.",
      "sampleStrongResponse": "Introduce WIP limit of 2 per developer; measure lead time and carryover reduction across next 3 sprints."
    },
    {
      "id": "10",
      "type": "freeform",
      "points": 4,
      "question": "Explain how you ensure Definition of Done is applied consistently across stories.",
      "sampleStrongResponse": "Shared DoD checklist in PR template + automated checks (tests, coverage, lint) in CI; audit sample of completed stories each sprint."
    }
  ]
},
  "capacity-planning": {
  "title": "Capacity Planning Knowledge Quiz",
  "totalQuestions": 10,
  "totalPoints": 25,
  "questions": [
    {
      "id": "1",
      "type": "multiple-choice",
      "points": 2,
      "question": "Velocity is best used as:",
      "options": [
        "A target teams must hit",
        "A comparison metric across teams",
        "A performance rating for individuals",
        "An input for team forecasting and planning"
      ],
      "correctAnswer": 3,
      "additionalContext": "Velocity is a team-internal trend to inform forecasts; it is not a target or comparison metric.",
      "keyConcepts": [
        "Velocity",
        "Forecasting",
        "Anti patterns"
      ]
    },
    {
      "id": "2",
      "type": "multiple-choice",
      "points": 2,
      "question": "Utilization near 100% typically leads to:",
      "options": [
        "Lower cycle time and faster flow",
        "Higher wait times and queueing delays",
        "Fewer incidents",
        "More flexibility and buffer"
      ],
      "correctAnswer": 1,
      "additionalContext": "High utilization creates queues and delays; leaving buffer improves predictability.",
      "keyConcepts": [
        "Utilization",
        "Queues",
        "Predictability"
      ]
    },
    {
      "id": "3",
      "type": "multiple-choice",
      "points": 2,
      "question": "A practical buffer for unplanned work in sprint forecasting:",
      "options": [
        "0% so we maximize commitment",
        "10&ndash;20% based on historical interrupts",
        "50% by default",
        "Only if leadership asks for it"
      ],
      "correctAnswer": 1,
      "additionalContext": "Reserve a modest buffer based on history to absorb support and discovery without frequent rollover.",
      "keyConcepts": [
        "Buffer",
        "Unplanned work",
        "Forecasting"
      ]
    },
    {
      "id": "4",
      "type": "multiple-choice",
      "points": 2,
      "question": "Forecast ranges communicate:",
      "options": [
        "Certainty of a single date",
        "A desire to avoid accountability",
        "Confidence intervals that reflect variability",
        "Only worst case scenarios"
      ],
      "correctAnswer": 2,
      "additionalContext": "Use ranges and probabilities to reflect uncertainty and set better expectations.",
      "keyConcepts": [
        "Ranges",
        "Confidence",
        "Uncertainty"
      ]
    },
    {
      "id": "5",
      "type": "multiple-choice",
      "points": 2,
      "question": "Variance in throughput or cycle time should prompt:",
      "options": [
        "Root cause exploration and scenario updates",
        "Team comparison for competition",
        "Immediate scope increase",
        "Ignoring the data to avoid churn"
      ],
      "correctAnswer": 0,
      "additionalContext": "Investigate sources of variance and update scenarios and buffers accordingly.",
      "keyConcepts": [
        "Variance",
        "Root cause",
        "Scenario updates"
      ]
    },
    {
      "id": "6",
      "type": "multiple-choice",
      "points": 2,
      "question": "Scenario planning in capacity planning means:",
      "options": [
        "Choosing one plan and sticking to it",
        "Deferring decisions until the deadline",
        "Building multiple plausible plans with triggers to switch",
        "Always picking the most optimistic plan"
      ],
      "correctAnswer": 2,
      "additionalContext": "Define best/base/worst cases with signals that indicate when to pivot between them.",
      "keyConcepts": [
        "Scenario planning",
        "Triggers",
        "Pivoting"
      ]
    },
    {
      "id": "7",
      "type": "multiple-choice",
      "points": 2,
      "question": "How should historical velocity be combined with upcoming constraints?",
      "options": [
        "Ignore constraints if velocity is high",
        "Keep the highest observed velocity as the commitment",
        "Use the manager&rsquo;s preference",
        "Adjust forecasts for holidays, support load, and dependencies"
      ],
      "correctAnswer": 3,
      "additionalContext": "Apply known constraints and expected interrupts to adjust from historical trends.",
      "keyConcepts": [
        "Historical data",
        "Constraints",
        "Adjustments"
      ]
    },
    {
      "id": "8",
      "type": "multiple-choice",
      "points": 2,
      "question": "A signal that a team is overcommitting in planning:",
      "options": [
        "Frequent rollover and growing WIP",
        "Stable cycle time and minimal carryover",
        "Predictable forecasts within range",
        "Consistent buffer usage without spillover"
      ],
      "correctAnswer": 0,
      "additionalContext": "Rollover and growing WIP indicate overcommitment; reduce scope and increase slice thinness.",
      "keyConcepts": [
        "Overcommitment",
        "WIP",
        "Rollover"
      ]
    },
    {
      "id": "9",
      "type": "freeform",
      "points": 4,
      "question": "You have 6 sprints of data with velocities: 22, 18, 24, 20, 21, 19. Propose a forecast range and buffer for the next sprint, and explain how you would communicate confidence.",
      "sampleStrongResponse": "Use recent range 18&ndash;24 with median near 20&ndash;21. Plan near the lower bound minus expected interrupts, e.g., 18&ndash;20 of planned work with a small buffer. Communicate as a range with confidence and assumptions, and revisit mid sprint with signals."
    },
    {
      "id": "10",
      "type": "freeform",
      "points": 5,
      "question": "A leadership date is proposed that assumes zero interrupts. Outline a scenario plan that includes triggers to tighten scope or switch tracks while maintaining quality.",
      "sampleStrongResponse": "Create base/best/worst scenarios with explicit buffers and quality guardrails. Define triggers like support ticket volume or dependency slippage to tighten scope, defer lower value items, or switch to a fallback plan. Keep quality bars enforced by CI and DoD."
    }
  ]
},
  "change-management": {
  "title": "Change Management Knowledge Quiz",
  "totalQuestions": 10,
  "totalPoints": 25,
  "questions": [
    {
      "id": "1",
      "type": "multiple-choice",
      "points": 2,
      "question": "Approvals vs guardrails &mdash; which is the mature stance?",
      "options": [
        "Shift routine, low&ndash;risk changes to automated guardrails; reserve approvals for high risk",
        "Require approvals for all changes regardless of risk",
        "Eliminate guardrails and rely on manager sign&ndash;off",
        "Use approvals only after incidents"
      ],
      "correctAnswer": 0,
      "additionalContext": "Mature orgs automate safety checks (&ldquo;policy&ndash;as&ndash;code&rdquo;) and retain approvals for novel or high&ndash;risk work.",
      "keyConcepts": [
        "Approvals",
        "Guardrails",
        "Policy&ndash;as&ndash;code"
      ]
    },
    {
      "id": "2",
      "type": "multiple-choice",
      "points": 2,
      "question": "Risk assessment inputs emphasized in the article?",
      "options": [
        "Impact, likelihood, detectability; consider seasonality and customer segments",
        "Aesthetics, novelty, brand color",
        "Developer preference only",
        "Cost of hardware upgrades"
      ],
      "correctAnswer": 0,
      "additionalContext": "Assess impact, likelihood, and detectability; include context like holiday traffic spikes and affected segments.",
      "keyConcepts": [
        "Risk assessment",
        "Detectability",
        "Seasonality"
      ]
    },
    {
      "id": "3",
      "type": "multiple-choice",
      "points": 2,
      "question": "Purpose of change windows?",
      "options": [
        "Concentrate staffing/comms when risk appetite is higher; keep emergency windows for true urgent work",
        "Spread staffing thinly across time",
        "Guarantee zero incidents",
        "Replace guardrails with manual reviews"
      ],
      "correctAnswer": 0,
      "additionalContext": "Change windows coordinate people and communication for planned risk, while emergency criteria avoid blanket freezes.",
      "keyConcepts": [
        "Change windows",
        "Risk appetite",
        "Emergency windows"
      ]
    },
    {
      "id": "4",
      "type": "multiple-choice",
      "points": 2,
      "question": "Communication and audit essentials for significant changes include:",
      "options": [
        "Pre&ndash;announce timelines/mitigations; keep who/what/when/why in audit trails",
        "Verbal updates only",
        "Hide impact until after rollout",
        "Delete records after success"
      ],
      "correctAnswer": 0,
      "additionalContext": "Communicate early and maintain auditability: record approvers, rationale, timing, and link to artifacts/incidents.",
      "keyConcepts": [
        "Communication plan",
        "Audit trail",
        "Stakeholders"
      ]
    },
    {
      "id": "5",
      "type": "multiple-choice",
      "points": 2,
      "question": "Change Advisory Board (CAB) usage per the article?",
      "options": [
        "Use for complex, multi&ndash;system changes; avoid for routine reversible work with strong guardrails",
        "Use for all production changes",
        "Avoid entirely",
        "Only convene after outages"
      ],
      "correctAnswer": 0,
      "additionalContext": "CABs help coordinate cross&ndash;system change; they are overkill for routine changes where guardrails suffice.",
      "keyConcepts": [
        "CAB",
        "Cross&ndash;team coordination",
        "Guardrails"
      ]
    },
    {
      "id": "6",
      "type": "multiple-choice",
      "points": 2,
      "question": "What increases safe throughput according to the article?",
      "options": [
        "Guardrails enabling smaller, more frequent changes",
        "Bigger batches and rarer releases",
        "Eliminating monitoring during windows",
        "Unlimited parallel high&ndash;risk changes"
      ],
      "correctAnswer": 0,
      "additionalContext": "Smaller, frequent changes with automated checks reduce batch risk and increase safe throughput.",
      "keyConcepts": [
        "Throughput",
        "Batch size",
        "Guardrails"
      ]
    },
    {
      "id": "7",
      "type": "multiple-choice",
      "points": 2,
      "question": "Auditability by default means:",
      "options": [
        "Collect approvals, evidence, and outcomes in one system; link to monitoring and tickets",
        "Keep approvals separate from outcomes",
        "Store records in personal notes",
        "Only log issues, not changes"
      ],
      "correctAnswer": 0,
      "additionalContext": "Centralize change records with links to PRs, deploys, incidents, RCAs, and monitoring annotations.",
      "keyConcepts": [
        "Auditability",
        "Single source of truth",
        "Integrations"
      ]
    },
    {
      "id": "8",
      "type": "multiple-choice",
      "points": 2,
      "question": "When replacing approvals with guardrails, teams should:",
      "options": [
        "Map risk tiers to guardrails and keep human review for high&ndash;impact or irreversible changes",
        "Remove all human review for novel changes",
        "Use guardrails only for staging",
        "Decide informally per engineer"
      ],
      "correctAnswer": 0,
      "additionalContext": "Codify risk tiers and align guardrails/approval paths; preserve human review for high&ndash;impact or irreversible changes.",
      "keyConcepts": [
        "Risk tiers",
        "Guardrails mapping",
        "Approvals"
      ]
    },
    {
      "id": "9",
      "type": "freeform",
      "points": 4,
      "question": "Classify a database index addition during peak season. Propose risk tier, guardrails vs approvals, change window, and communication artifacts.",
      "sampleStrongResponse": "Risk: medium&ndash;high due to peak traffic and potential latency impact. Guardrails: tests, migration dry&ndash;run on prod&ndash;like data, capacity checks, monitoring annotations, rollback plan. Approvals: domain owner + DBA. Change window: staffed period with on&ndash;call ack. Comms: pre&ndash;announce internal impact, link to runbook and rollback triggers; update audit record with outcomes."
    },
    {
      "id": "10",
      "type": "freeform",
      "points": 5,
      "question": "Design an end&ndash;to&ndash;end change record template that satisfies audit trail essentials and streamlines comms. Include how low&ndash;risk changes self&ndash;approve when guardrails pass.",
      "sampleStrongResponse": "Template fields: what/why, risk tier, guardrails evidence (tests, SLO checks, rollout/rollback plan), approvers (auto&ndash;assigned by domain), change window, monitoring annotations, links to PRs/flags/deploys, and outcomes/RCAs. Policy: if risk tier is low and all guardrails pass, auto self&ndash;approve with owner ack; otherwise require listed approvers. System auto&ndash;publishes comms to stakeholders and stores immutable logs."
    }
  ]
},
  "code-editors-vs-ides": {
  "title": "Code Editors vs IDEs Knowledge Quiz",
  "totalQuestions": 10,
  "totalPoints": 25,
  "questions": [
    {
      "id": "1",
      "type": "multiple-choice",
      "points": 2,
      "question": "What is the core difference between a lightweight code editor and a full IDE?",
      "options": [
        "IDEs integrate debugging, refactoring, and project tooling; editors focus on editing with optional plugins",
        "Editors always include full build systems and test runners",
        "IDEs cannot be extended with plugins",
        "Editors are only for plain‑text files, not code"
      ],
      "correctAnswer": 0,
      "additionalContext": "Editors prioritize startup speed and small footprint. IDEs ship integrated debugging, refactors, test runners, and project models out of the box for deeper workflows.",
      "keyConcepts": [
        "Capability vs footprint",
        "Integration depth",
        "Extensibility"
      ]
    },
    {
      "id": "2",
      "type": "multiple-choice",
      "points": 2,
      "question": "Which statement best describes typical startup performance?",
      "options": [
        "IDEs always start faster due to indexing",
        "Editors tend to start faster; IDEs may take longer due to indexing and project scanning",
        "Editors are always slower because they lack features",
        "Both are identical in startup time"
      ],
      "correctAnswer": 1,
      "additionalContext": "Indexing and heavy project models can add startup overhead in IDEs, while editors often defer work until a feature is invoked.",
      "keyConcepts": [
        "Startup time",
        "Indexing",
        "Deferred work"
      ]
    },
    {
      "id": "3",
      "type": "multiple-choice",
      "points": 2,
      "question": "How can plugin ecosystems affect capability parity between editors and IDEs?",
      "options": [
        "Plugins rarely add significant features",
        "Plugins only change themes",
        "Rich plugin ecosystems can close capability gaps, though configuration effort may rise",
        "Plugins eliminate the need for version control"
      ],
      "correctAnswer": 2,
      "additionalContext": "With the right extensions (LSP, debugger adapters, test runners), editors can approximate IDE features but require careful setup.",
      "keyConcepts": [
        "Plugin ecosystems",
        "Configuration",
        "LSP"
      ]
    },
    {
      "id": "4",
      "type": "multiple-choice",
      "points": 2,
      "question": "What is a primary benefit of remote development (for example, dev containers or remote SSH) in this context?",
      "options": [
        "It guarantees zero latency",
        "It prevents any dependency drift automatically",
        "It makes local CPUs irrelevant in all cases",
        "It offloads heavy toolchains to a remote host while using a local UI"
      ],
      "correctAnswer": 3,
      "additionalContext": "Remote development centralizes toolchains and compute in a consistent environment while the editor/IDE provides the interface locally.",
      "keyConcepts": [
        "Remote dev",
        "Dev containers",
        "Consistency"
      ]
    },
    {
      "id": "5",
      "type": "multiple-choice",
      "points": 2,
      "question": "When might a lightweight editor be preferable over an IDE for a large monorepo?",
      "options": [
        "When deep refactors across many projects are required immediately",
        "When you need built‑in database migration tools",
        "When quick edits are needed and heavy indexing would slow you down",
        "When you must compile native toolchains locally"
      ],
      "correctAnswer": 2,
      "additionalContext": "For quick, localized changes, avoiding full indexing can reduce waiting time. For large cross‑cutting changes, IDE capabilities may win.",
      "keyConcepts": [
        "Monorepos",
        "Indexing cost",
        "Edit latency"
      ]
    },
    {
      "id": "6",
      "type": "multiple-choice",
      "points": 2,
      "question": "In which scenario does an IDE typically provide outsized benefits?",
      "options": [
        "Editing a single config file",
        "Performing large, type‑aware refactors and deep navigation in complex codebases",
        "Previewing Markdown",
        "Viewing logs only"
      ],
      "correctAnswer": 1,
      "additionalContext": "Type‑aware navigation and refactoring tools shine in big, strongly typed codebases where correctness and consistency matter.",
      "keyConcepts": [
        "Refactors",
        "Type awareness",
        "Deep navigation"
      ]
    },
    {
      "id": "7",
      "type": "multiple-choice",
      "points": 2,
      "question": "What is a common risk of installing too many extensions?",
      "options": [
        "Increased CPU, memory usage, and slower startup times",
        "Lower memory usage",
        "Guaranteed stability improvements",
        "Automatic security hardening"
      ],
      "correctAnswer": 0,
      "additionalContext": "Each extension can load background processes, watchers, or parsers that add overhead. A curated set helps maintain performance.",
      "keyConcepts": [
        "Extension bloat",
        "Performance budgets",
        "Startup"
      ]
    },
    {
      "id": "8",
      "type": "multiple-choice",
      "points": 2,
      "question": "What is a typical trade‑off of remote development compared to fully local development?",
      "options": [
        "No need for security reviews",
        "Unlimited offline capability",
        "Zero cost for compute",
        "Potential latency and dependency on network connectivity"
      ],
      "correctAnswer": 3,
      "additionalContext": "While remote environments can be consistent and powerful, interactive tasks can feel slower on poor connections.",
      "keyConcepts": [
        "Latency",
        "Connectivity",
        "Remote trade‑offs"
      ]
    },
    {
      "id": "9",
      "type": "freeform",
      "points": 4,
      "question": "Your team is deciding between an IDE‑first or editor‑plus‑plugins approach. Explain the trade‑offs and when you would recommend each.",
      "sampleStrongResponse": "Recommend an IDE for large, typed codebases where refactor safety, deep navigation, and integrated debugging reduce risk. Recommend an editor‑plus‑plugins for fast startup, lightweight machines, or polyglot teams where only a subset of features is needed. Note the setup tax for plugins and the need for extension governance. Consider remote development when local machines struggle with indexing.",
      "keyConcepts": [
        "Trade‑offs",
        "Team context",
        "Risk vs speed"
      ]
    },
    {
      "id": "10",
      "type": "freeform",
      "points": 5,
      "question": "Propose a plugin governance policy that balances capability with performance and security for your organization.",
      "sampleStrongResponse": "Define an allowlist with review criteria (maintenance cadence, permissions requested, reputation). Set performance budgets (startup time, memory) and monitor with profiling tools. Require &ldquo;least privilege&rdquo; by disabling unused capabilities. Stage updates in a pilot group before broad rollout, and document alternatives for any blocked extensions.",
      "keyConcepts": [
        "Governance",
        "Allowlist",
        "Performance budgets",
        "Security"
      ]
    }
  ]
},
  "code-reviews": {
  "title": "Code Reviews Knowledge Quiz",
  "totalQuestions": 10,
  "totalPoints": 25,
  "questions": [
    {
      "id": "1",
      "type": "multiple-choice",
      "points": 2,
      "question": "Primary goal of code reviews:",
      "options": [
        "Detect defects early and reduce risk",
        "Enforce individual style preferences",
        "Slow down delivery to catch all nits",
        "Replace testing entirely"
      ],
      "correctAnswer": 0,
      "additionalContext": "Early reviews surface issues before production when remediation costs can rise 10x–100x; they also spread knowledge.",
      "keyConcepts": [
        "Defect detection",
        "Risk reduction",
        "Knowledge sharing"
      ]
    },
    {
      "id": "2",
      "type": "multiple-choice",
      "points": 2,
      "question": "Recommended review batch size:",
      "options": [
        "Thousands of lines per PR",
        "Small, focused changes with clear intent",
        "Only one PR per quarter",
        "Massive refactors mixed with unrelated changes"
      ],
      "correctAnswer": 1,
      "additionalContext": "Smaller, focused PRs reduce cognitive load and increase feedback quality and speed.",
      "keyConcepts": [
        "Batch size",
        "Focused PRs",
        "Cognitive load"
      ]
    },
    {
      "id": "3",
      "type": "multiple-choice",
      "points": 2,
      "question": "Best review style for high‑risk, complex changes:",
      "options": [
        "Async PR review only",
        "No review if tests pass",
        "Pair review (synchronous walkthrough)",
        "Wait until after release"
      ],
      "correctAnswer": 2,
      "additionalContext": "Pairing on complex changes improves shared understanding and catches design issues earlier.",
      "keyConcepts": [
        "Pair review",
        "Complex changes",
        "Risk"
      ]
    },
    {
      "id": "4",
      "type": "multiple-choice",
      "points": 2,
      "question": "Feedback norm aligned with psychological safety:",
      "options": [
        "Gatekeep with blanket rejections",
        "Block on preference‑only comments",
        "Nitpick unrelated formatting",
        "Ask questions with rationale and offer alternatives"
      ],
      "correctAnswer": 3,
      "additionalContext": "Prefer questions and rationale (&ldquo;Could we extract this because...&rdquo;) and separate must‑fix issues from suggestions.",
      "keyConcepts": [
        "Psychological safety",
        "Feedback norms",
        "Must‑fix vs suggestions"
      ]
    },
    {
      "id": "5",
      "type": "multiple-choice",
      "points": 2,
      "question": "Useful reviewer checklist focus:",
      "options": [
        "Personal naming preferences",
        "Keyboard layout consistency",
        "Security, accessibility, performance budgets",
        "Whimsical style changes"
      ],
      "correctAnswer": 2,
      "additionalContext": "Checklists tailored to code areas (security, accessibility, performance, error handling) improve consistency.",
      "keyConcepts": [
        "Checklists",
        "Quality gates",
        "Consistency"
      ]
    },
    {
      "id": "6",
      "type": "multiple-choice",
      "points": 2,
      "question": "Expected outcomes of effective reviews:",
      "options": [
        "Longer cycle times",
        "Higher change success rate and predictable cycle time",
        "More rollbacks",
        "Less onboarding clarity"
      ],
      "correctAnswer": 1,
      "additionalContext": "Outcomes include fewer production defects, faster onboarding, and steadier delivery.",
      "keyConcepts": [
        "Outcomes",
        "Change success",
        "Cycle time"
      ]
    },
    {
      "id": "7",
      "type": "multiple-choice",
      "points": 2,
      "question": "Which is an anti‑pattern in reviews?",
      "options": [
        "Rubber‑stamping large diffs without context",
        "Providing context and standards links",
        "Clarifying must‑fix vs suggestions",
        "Keeping PRs focused"
      ],
      "correctAnswer": 0,
      "additionalContext": "Avoid rubber‑stamping and preference‑only blocking; focus on risk hot spots and clarity.",
      "keyConcepts": [
        "Anti‑patterns",
        "Rubber‑stamping"
      ]
    },
    {
      "id": "8",
      "type": "multiple-choice",
      "points": 2,
      "question": "Policy‑as‑code guardrails primarily:",
      "options": [
        "Replace human reviews",
        "Measure keyboard speed",
        "Enforce security/performance policies via linters and CI",
        "Delay releases by adding manual steps"
      ],
      "correctAnswer": 3,
      "additionalContext": "Automated checks enforce standards and surface breaking changes to APIs and contracts early.",
      "keyConcepts": [
        "Policy‑as‑code",
        "Linters",
        "CI"
      ]
    },
    {
      "id": "9",
      "type": "freeform",
      "points": 4,
      "question": "Describe how you would structure a reviewer checklist for a critical area (e.g., auth or data access) and how you would validate its effectiveness over time.",
      "sampleStrongResponse": "Create a short, risk‑based checklist (input validation, authz, logging, error handling, perf budgets). Track escaped defects and change failure rate; spot‑audit PRs monthly and evolve items when incidents occur."
    },
    {
      "id": "10",
      "type": "freeform",
      "points": 5,
      "question": "A teammate flags that reviews feel like gates rather than collaboration. How would you reset norms and measure improvement?",
      "sampleStrongResponse": "Run a brief workshop to align on goals and examples of constructive feedback; update PR template to separate must‑fix vs suggestions and rationale. Measure review turnaround, change success rate, and developer sentiment over 2–3 sprints."
    }
  ]
},
  "developer-productivity-tools": {
  "title": "Developer Productivity Tools Knowledge Quiz",
  "totalQuestions": 10,
  "totalPoints": 25,
  "questions": [
    {
      "id": "1",
      "type": "multiple-choice",
      "points": 2,
      "question": "What is the primary distinction between linters and code formatters?",
      "options": [
        "Formatters detect security vulnerabilities; linters only change whitespace",
        "Linters only add colors; formatters enforce naming conventions",
        "Linters enforce rules and surface potential defects; formatters standardize code style",
        "They are interchangeable tools"
      ],
      "correctAnswer": 2,
      "additionalContext": "Linters flag probable bugs and policy violations (for example, unused variables), while formatters normalize layout so developers focus on logic, not style.",
      "keyConcepts": [
        "Linters",
        "Formatters",
        "Quality vs style"
      ]
    },
    {
      "id": "2",
      "type": "multiple-choice",
      "points": 2,
      "question": "How do pre‑commit hooks improve consistency?",
      "options": [
        "They randomly block commits",
        "They auto‑merge branches",
        "They enforce checks (for example, lint/format) before code lands",
        "They remove tests to speed up CI"
      ],
      "correctAnswer": 0,
      "additionalContext": "Pre‑commit hooks run tooling locally (format, lint, type checks) to catch issues early and reduce churn in CI.",
      "keyConcepts": [
        "Pre‑commit",
        "Consistency",
        "Shift‑left"
      ]
    },
    {
      "id": "3",
      "type": "multiple-choice",
      "points": 2,
      "question": "What advantage does structural code search provide over plain text search?",
      "options": [
        "It searches only comments",
        "It matches code patterns at the syntax/AST level (for example, function calls with certain arguments)",
        "It renames files automatically",
        "It replaces the need for tests"
      ],
      "correctAnswer": 1,
      "additionalContext": "Structural search understands code shapes, enabling precise queries like finding unsafe API usages versus broad text matches.",
      "keyConcepts": [
        "Structural search",
        "AST",
        "Precision"
      ]
    },
    {
      "id": "4",
      "type": "multiple-choice",
      "points": 2,
      "question": "How do snippets and macros boost developer throughput?",
      "options": [
        "By disabling type checking",
        "By hiding errors",
        "By increasing network bandwidth",
        "By automating repetitive patterns and reducing keystrokes"
      ],
      "correctAnswer": 3,
      "additionalContext": "Reusable templates for boilerplate (for example, test skeletons) reduce friction and promote consistency.",
      "keyConcepts": [
        "Snippets",
        "Automation",
        "Consistency"
      ]
    },
    {
      "id": "5",
      "type": "multiple-choice",
      "points": 2,
      "question": "What role do task runners (for example, npm scripts, Make, or Invoke) play?",
      "options": [
        "They replace version control",
        "They handle production incident response",
        "They provide a single entry point to common commands and workflows",
        "They automatically refactor code"
      ],
      "correctAnswer": 2,
      "additionalContext": "Task runners standardize commands like test, lint, build, and release, reducing the &ldquo;how do I run this&rdquo; learning curve.",
      "keyConcepts": [
        "Task runners",
        "Standardization",
        "DX"
      ]
    },
    {
      "id": "6",
      "type": "multiple-choice",
      "points": 2,
      "question": "Why is terminal/CLI integration inside the editor/IDE useful?",
      "options": [
        "It disables environment variables",
        "It forces GUI usage only",
        "It removes the need for build tools",
        "It keeps commands, environment, and output close to the code for fast iteration"
      ],
      "correctAnswer": 3,
      "additionalContext": "An embedded terminal preserves project context and makes iterative loops (edit‑run‑test) quicker.",
      "keyConcepts": [
        "CLI",
        "Feedback loop",
        "Context switching"
      ]
    },
    {
      "id": "7",
      "type": "multiple-choice",
      "points": 2,
      "question": "How should Prettier and ESLint typically be used together?",
      "options": [
        "Run Prettier for styling and use ESLint for rules; resolve conflicts with appropriate configs",
        "Only run ESLint and ignore formatting",
        "Run both but disable all rules",
        "Run Prettier as a linter plugin so no configuration is needed"
      ],
      "correctAnswer": 0,
      "additionalContext": "Prettier handles formatting concerns, while ESLint enforces quality and policy rules. Use configs to avoid rule overlap.",
      "keyConcepts": [
        "Prettier",
        "ESLint",
        "Tooling harmony"
      ]
    },
    {
      "id": "8",
      "type": "multiple-choice",
      "points": 2,
      "question": "What is a balanced policy for linter severity in CI?",
      "options": [
        "Fail CI on any warning",
        "Fail on errors while tracking warnings; escalate categories over time",
        "Ignore errors and warnings",
        "Fail only on style issues"
      ],
      "correctAnswer": 1,
      "additionalContext": "Start with errors as blockers and warnings as tracked metrics, then ratchet up standards as the codebase improves.",
      "keyConcepts": [
        "CI policy",
        "Severity",
        "Continuous improvement"
      ]
    },
    {
      "id": "9",
      "type": "freeform",
      "points": 4,
      "question": "Outline a plan to roll out linters and formatters to an existing repository with minimal disruption.",
      "sampleStrongResponse": "Introduce shared configs and run tools in fix mode to create a baseline commit. Add pre‑commit hooks to prevent regressions. In CI, fail on errors and report warnings. Communicate style decisions and provide editor integration steps. Apply changes in focused batches to avoid noisy diffs and coordinate with active feature branches.",
      "keyConcepts": [
        "Migration plan",
        "Hooks",
        "Baseline commit"
      ]
    },
    {
      "id": "10",
      "type": "freeform",
      "points": 5,
      "question": "Design a productivity toolkit for a new service: specify search tools, snippets, tasks, and CI integrations that will reduce onboarding time.",
      "sampleStrongResponse": "Provide structural code search patterns for common APIs, a curated snippet set for test and handler templates, and npm scripts for dev, test, lint, and type‑check. Add a &ldquo;first‑run&rdquo; script to install dependencies and pre‑commit hooks. Document how to run everything inside the editor&rsquo;s terminal so newcomers can ship a passing change in under an hour.",
      "keyConcepts": [
        "Onboarding",
        "Toolkit",
        "Standard scripts"
      ]
    }
  ]
},
  "devops-philosophy": {
  "title": "DevOps Philosophy Knowledge Quiz",
  "totalQuestions": 10,
  "totalPoints": 24,
  "questions": [
    {
      "id": "1",
      "type": "multiple-choice",
      "points": 2,
      "question": "Core DevOps objective?",
      "options": [
        "Faster, safer delivery",
        "More meetings",
        "Quarterly releases",
        "Manual deployments"
      ],
      "correctAnswer": 0,
      "additionalContext": "DevOps optimizes delivery speed and reliability by aligning dev and ops through automation and collaboration.",
      "keyConcepts": [
        "Delivery",
        "Collaboration",
        "Automation"
      ]
    },
    {
      "id": "2",
      "type": "multiple-choice",
      "points": 2,
      "question": "Key DevOps practice for environment parity:",
      "options": [
        "Pet servers",
        "Manual config",
        "Immutable infrastructure",
        "Snowflake hosts"
      ],
      "correctAnswer": 2,
      "additionalContext": "Immutable infra ensures deployments replace rather than mutate servers, preventing drift.",
      "keyConcepts": [
        "Immutable Infra",
        "Drift"
      ]
    },
    {
      "id": "3",
      "type": "multiple-choice",
      "points": 2,
      "question": "CI best describes:",
      "options": [
        "Compiling once a year",
        "Merging infrequently",
        "Automated build/test per change",
        "Manual QA only"
      ],
      "correctAnswer": 2,
      "additionalContext": "Each change triggers build and tests, catching issues early and often.",
      "keyConcepts": [
        "CI",
        "Automation"
      ]
    },
    {
      "id": "4",
      "type": "multiple-choice",
      "points": 2,
      "question": "CD focuses on:",
      "options": [
        "Long release trains",
        "Automated, reliable releases",
        "Manual approvals only",
        "Freeze windows"
      ],
      "correctAnswer": 1,
      "additionalContext": "CD automates release pipelines for consistent, low‑risk deployments.",
      "keyConcepts": [
        "CD",
        "Pipelines"
      ]
    },
    {
      "id": "5",
      "type": "multiple-choice",
      "points": 2,
      "question": "Observability provides:",
      "options": [
        "Feature flags",
        "Build caching",
        "System insights via metrics/logs/traces",
        "Static analysis"
      ],
      "correctAnswer": 2,
      "additionalContext": "Observability instruments systems to understand internal state via signals and traces.",
      "keyConcepts": [
        "Observability",
        "Telemetry"
      ]
    },
    {
      "id": "6",
      "type": "multiple-choice",
      "points": 2,
      "question": "Infrastructure as Code benefit:",
      "options": [
        "Undocumented state",
        "Drift tolerance",
        "Reproducibility and reviewability",
        "Manual hotfixes only"
      ],
      "correctAnswer": 2,
      "additionalContext": "IaC makes infra changes declarative, versioned, and reviewable like code.",
      "keyConcepts": [
        "IaC",
        "Reproducibility"
      ]
    },
    {
      "id": "7",
      "type": "multiple-choice",
      "points": 2,
      "question": "Change failure rate goal:",
      "options": [
        "As high as possible",
        "Unknown",
        "As low as possible",
        "Exactly 10%"
      ],
      "correctAnswer": 2,
      "additionalContext": "Elite DevOps performance targets low change failure rate alongside high frequency and fast recovery.",
      "keyConcepts": [
        "DORA",
        "CFR"
      ]
    },
    {
      "id": "8",
      "type": "multiple-choice",
      "points": 2,
      "question": "Deployment strategy reducing blast radius:",
      "options": [
        "Big bang",
        "All-at-once",
        "Blue/Green or Canary",
        "Manual midnight release"
      ],
      "correctAnswer": 2,
      "additionalContext": "Blue/Green and canary isolate impact and allow quick rollback.",
      "keyConcepts": [
        "Blue/Green",
        "Canary",
        "Rollback"
      ]
    },
    {
      "id": "9",
      "type": "freeform",
      "points": 4,
      "question": "Name two metrics you would track to measure DevOps success and why they matter.",
      "sampleStrongResponse": "Lead time for changes and change failure rate; together reflect delivery speed and stability with direct business impact."
    },
    {
      "id": "10",
      "type": "freeform",
      "points": 4,
      "question": "Describe a rollback plan for a critical service and how you would test it.",
      "sampleStrongResponse": "Use Blue/Green with traffic switch and database safety checks; practice regular game days validating scripts and monitoring thresholds."
    }
  ]
},
  "documentation-standards": {
  "title": "Documentation Standards Knowledge Quiz",
  "totalQuestions": 10,
  "totalPoints": 25,
  "questions": [
    {
      "id": "1",
      "type": "multiple-choice",
      "points": 2,
      "question": "Docs‑as‑Code emphasizes:",
      "options": [
        "Ad‑hoc wikis without reviews",
        "Versioned docs reviewed via PRs and validated by CI",
        "Docs separate from source control",
        "Handwritten notes only"
      ],
      "correctAnswer": 1,
      "additionalContext": "Treat docs like code: versioning, reviews, and CI validation improve quality and freshness.",
      "keyConcepts": [
        "Docs‑as‑Code",
        "PR reviews",
        "CI validation"
      ]
    },
    {
      "id": "2",
      "type": "multiple-choice",
      "points": 2,
      "question": "Templates help by:",
      "options": [
        "Enforcing required sections and style",
        "Replacing all documentation",
        "Hiding ownership information",
        "Preventing updates"
      ],
      "correctAnswer": 0,
      "additionalContext": "ADRs, runbooks, and README templates ensure consistent structure and content.",
      "keyConcepts": [
        "Templates",
        "Consistency",
        "Quality"
      ]
    },
    {
      "id": "3",
      "type": "multiple-choice",
      "points": 2,
      "question": "Information architecture practice:",
      "options": [
        "Unclear ownership and nested folders without logic",
        "Random links between services",
        "Clear ownership and discoverability per system or domain",
        "One giant README for everything"
      ],
      "correctAnswer": 2,
      "additionalContext": "Link code to docs (services → READMEs, APIs → OpenAPI, ops → runbooks) with clear owners.",
      "keyConcepts": [
        "Ownership",
        "Discoverability",
        "Linking code to docs"
      ]
    },
    {
      "id": "4",
      "type": "multiple-choice",
      "points": 2,
      "question": "Governance that prevents drift:",
      "options": [
        "No review cadence",
        "Annual audits only",
        "Ongoing review cadence and freshness SLAs",
        "Delete stale docs without replacement"
      ],
      "correctAnswer": 2,
      "additionalContext": "Set owners and review cadence to keep documentation current and trustworthy.",
      "keyConcepts": [
        "Governance",
        "Freshness",
        "SLA"
      ]
    },
    {
      "id": "5",
      "type": "multiple-choice",
      "points": 2,
      "question": "Metric that indicates documentation health:",
      "options": [
        "Number of emojis used",
        "Doc freshness (time since last verified)",
        "Team size",
        "Lines of code in the repo"
      ],
      "correctAnswer": 1,
      "additionalContext": "Track freshness, coverage, and search success/time‑to‑find to improve outcomes.",
      "keyConcepts": [
        "Freshness",
        "Coverage",
        "Search success"
      ]
    },
    {
      "id": "6",
      "type": "multiple-choice",
      "points": 2,
      "question": "Linking code to docs example:",
      "options": [
        "APIs → OpenAPI specs; operations → runbooks",
        "APIs → random chat screenshots",
        "Services → unrelated blog posts",
        "Incidents → no records"
      ],
      "correctAnswer": 0,
      "additionalContext": "Make APIs discoverable through OpenAPI; operational knowledge through runbooks; services through READMEs.",
      "keyConcepts": [
        "OpenAPI",
        "Runbooks",
        "READMEs"
      ]
    },
    {
      "id": "7",
      "type": "multiple-choice",
      "points": 2,
      "question": "Policy/style linting in CI helps by:",
      "options": [
        "Blocking all documentation",
        "Automating checks for required sections and style",
        "Randomly reformatting content",
        "Replacing human review fully"
      ],
      "correctAnswer": 1,
      "additionalContext": "Linting detects missing sections and style drift early, improving quality.",
      "keyConcepts": [
        "Linting",
        "CI",
        "Quality"
      ]
    },
    {
      "id": "8",
      "type": "multiple-choice",
      "points": 2,
      "question": "Outcome of robust documentation standards:",
      "options": [
        "Increased coordination cost",
        "More blocked incidents",
        "Reduced handoff errors and faster onboarding",
        "No change to audit readiness"
      ],
      "correctAnswer": 2,
      "additionalContext": "Standards reduce handoff errors, unblock incidents, and improve compliance readiness.",
      "keyConcepts": [
        "Handoffs",
        "Onboarding",
        "Compliance"
      ]
    },
    {
      "id": "9",
      "type": "freeform",
      "points": 4,
      "question": "Propose a &ldquo;docs‑as‑code&rdquo; rollout for a monorepo: templates, ownership, CI checks, and review cadence. How will you measure improvement?",
      "sampleStrongResponse": "Introduce README, runbook, and ADR templates with owners per service. Add CI linting for required sections and stale checks. Establish quarterly freshness reviews. Measure doc freshness, coverage across systems, search success rate, and mean time‑to‑find critical docs."
    },
    {
      "id": "10",
      "type": "freeform",
      "points": 5,
      "question": "An audit is upcoming. Outline a lightweight plan to achieve compliance readiness by default through documentation standards and automation.",
      "sampleStrongResponse": "Define ownership and SLAs, enforce templates, and add CI checks for required content. Link code to OpenAPI and runbooks, auto‑summarize incidents/PRs into docs, and monitor coverage/freshness dashboards. Success is smoother audits and reduced time‑to‑find during incidents."
    }
  ]
},
  "extension-ecosystems": {
  "title": "Extension Ecosystems Knowledge Quiz",
  "totalQuestions": 10,
  "totalPoints": 25,
  "questions": [
    {
      "id": "1",
      "type": "multiple-choice",
      "points": 2,
      "question": "What is a primary security risk introduced by editor/IDE extensions?",
      "options": [
        "Supply‑chain exposure if extensions have broad filesystem or network permissions",
        "Guaranteed elimination of all vulnerabilities",
        "Automatic code review of all commits",
        "Hardware isolation of the development machine"
      ],
      "correctAnswer": 0,
      "additionalContext": "Extensions may have access to files, environment variables, and network calls. Poorly vetted plugins can exfiltrate data or inject malicious code.",
      "keyConcepts": [
        "Supply chain",
        "Permissions",
        "Data exfiltration"
      ]
    },
    {
      "id": "2",
      "type": "multiple-choice",
      "points": 2,
      "question": "What is a good first‑pass evaluation signal when assessing an extension?",
      "options": [
        "Number of colors in its theme",
        "Presence of animated icons",
        "Maintenance cadence, clear permissions, and reputable publisher",
        "Total size of screenshots"
      ],
      "correctAnswer": 2,
      "additionalContext": "Look for an active changelog, transparent scopes/permissions, signed publishers, and community reputation before trial.",
      "keyConcepts": [
        "Evaluation",
        "Reputation",
        "Permissions"
      ]
    },
    {
      "id": "3",
      "type": "multiple-choice",
      "points": 2,
      "question": "How can teams isolate performance issues potentially caused by extensions?",
      "options": [
        "Install more extensions to mask the problem",
        "Ignore any slowdowns",
        "Disable the IDE entirely",
        "Use built‑in profiling and extension bisect/safe‑mode to identify the culprit"
      ],
      "correctAnswer": 3,
      "additionalContext": "Many tools provide an &ldquo;extension bisect&rdquo; or safe‑mode that disables subsets of extensions to quickly find regressions.",
      "keyConcepts": [
        "Performance diagnostics",
        "Profiling",
        "Bisect"
      ]
    },
    {
      "id": "4",
      "type": "multiple-choice",
      "points": 2,
      "question": "Which policy best addresses telemetry and privacy concerns with extensions?",
      "options": [
        "Allow any telemetry by default",
        "Review extension telemetry, disable unnecessary tracking, and document data flows",
        "Block all extensions regardless of function",
        "Trust publishers without review"
      ],
      "correctAnswer": 1,
      "additionalContext": "A balanced approach audits what data leaves developer machines and ensures collection aligns with company policy.",
      "keyConcepts": [
        "Telemetry",
        "Privacy",
        "Policy"
      ]
    },
    {
      "id": "5",
      "type": "multiple-choice",
      "points": 2,
      "question": "What is a best practice for update hygiene in an extension‑heavy environment?",
      "options": [
        "Auto‑update all extensions immediately in production",
        "Never update extensions",
        "Let each developer choose any version",
        "Stage updates with a canary group and pin versions for critical tools"
      ],
      "correctAnswer": 3,
      "additionalContext": "Staged rollouts catch regressions early. Pinning versions for critical tools prevents surprise breakages.",
      "keyConcepts": [
        "Update hygiene",
        "Staged rollout",
        "Version pinning"
      ]
    },
    {
      "id": "6",
      "type": "multiple-choice",
      "points": 2,
      "question": "What is the purpose of an organization extension allowlist?",
      "options": [
        "To improve keyboard backlighting",
        "To block all development",
        "To require administrative passwords for every keystroke",
        "To approve a curated set of extensions that meet security and performance standards"
      ],
      "correctAnswer": 0,
      "additionalContext": "An allowlist defines which extensions are approved. Criteria include maintenance, permissions, and performance impact.",
      "keyConcepts": [
        "Allowlist",
        "Governance",
        "Standards"
      ]
    },
    {
      "id": "7",
      "type": "multiple-choice",
      "points": 2,
      "question": "What principle should guide requested permissions for extensions?",
      "options": [
        "Request every permission to avoid friction",
        "Request the minimum necessary (least privilege)",
        "Avoid declaring permissions",
        "Share credentials for convenience"
      ],
      "correctAnswer": 1,
      "additionalContext": "Least privilege reduces blast radius if an extension is compromised and limits accidental data exposure.",
      "keyConcepts": [
        "Least privilege",
        "Permissions",
        "Risk reduction"
      ]
    },
    {
      "id": "8",
      "type": "multiple-choice",
      "points": 2,
      "question": "How should teams handle extensions in remote/containerized development environments?",
      "options": [
        "Allow any extension from the public marketplace",
        "Disable all extensions always",
        "Audit container capabilities and restrict extension hosts to the project scope",
        "Run extensions as root for convenience"
      ],
      "correctAnswer": 2,
      "additionalContext": "Scope extensions to the workspace folder, avoid privileged containers, and review shared images for preinstalled plugins.",
      "keyConcepts": [
        "Remote dev",
        "Containers",
        "Scope restrictions"
      ]
    },
    {
      "id": "9",
      "type": "freeform",
      "points": 4,
      "question": "Draft a lightweight process for evaluating and approving new extensions for team use.",
      "sampleStrongResponse": "Define review criteria (publisher reputation, maintenance cadence, permissions requested, telemetry behavior). Test the extension in a sandbox project, measure startup impact, and run a brief security review. If approved, add it to the allowlist with version pinning and document configuration steps.",
      "keyConcepts": [
        "Evaluation",
        "Sandboxing",
        "Approval"
      ]
    },
    {
      "id": "10",
      "type": "freeform",
      "points": 5,
      "question": "Describe how you would detect and remediate a performance regression introduced by an extension across a large team.",
      "sampleStrongResponse": "Use profiling and the platform&rsquo;s extension bisect to confirm the culprit. Roll back or pin the previous version, notify a pilot channel, and open an upstream issue with traces. Update the allowlist with a mitigation note and monitor startup and idle CPU metrics before re‑enabling the extension broadly.",
      "keyConcepts": [
        "Regression response",
        "Pin/rollback",
        "Monitoring"
      ]
    }
  ]
},
  "feature-flags": {
  "title": "Feature Flags Knowledge Quiz",
  "totalQuestions": 10,
  "totalPoints": 25,
  "questions": [
    {
      "id": "1",
      "type": "multiple-choice",
      "points": 2,
      "question": "Which statement correctly maps common flag types?",
      "options": [
        "Release flags gate incomplete work; ops flags toggle runtime behavior; experiment flags run A/B or multivariate tests",
        "Release flags for experiments; ops flags for A/B tests; experiment flags for incident response",
        "Ops flags hide WIP; experiment flags manage cache TTLs; release flags target cohorts",
        "All flag types are interchangeable if you log evaluations"
      ],
      "correctAnswer": 0,
      "additionalContext": "Release flags hide work&ndash;in&ndash;progress, ops flags change operational behavior, and experiment flags power A/B or multivariate tests.",
      "keyConcepts": [
        "Flag taxonomy",
        "Release",
        "Ops",
        "Experiment"
      ]
    },
    {
      "id": "2",
      "type": "multiple-choice",
      "points": 2,
      "question": "Best practice at flag creation time?",
      "options": [
        "Skip ownership to reduce process",
        "Set intent (&ldquo;release&rdquo;/&ldquo;ops&rdquo;/&ldquo;experiment&rdquo;), owner, and a remove&ndash;by date",
        "Default flags to permanent",
        "Use ad&ndash;hoc names and rely on memory"
      ],
      "correctAnswer": 1,
      "additionalContext": "Defining intent, ownership, and expiry up front prevents orphaned toggles and reduces future risk.",
      "keyConcepts": [
        "Lifecycle",
        "Ownership",
        "Expiry"
      ]
    },
    {
      "id": "3",
      "type": "multiple-choice",
      "points": 2,
      "question": "Safe default rollout pattern for a new high&ndash;risk flag?",
      "options": [
        "Flip globally immediately",
        "Target only new users worldwide",
        "Ramp from internal/staff to small percentages, then cohorts or regions",
        "Roll out by random servers without monitoring"
      ],
      "correctAnswer": 2,
      "additionalContext": "Start with staff/internal exposure, then percentage ramps or cohort/geo targeting to limit blast radius while learning.",
      "keyConcepts": [
        "Progressive delivery",
        "Targeting",
        "Blast radius"
      ]
    },
    {
      "id": "4",
      "type": "multiple-choice",
      "points": 2,
      "question": "A &ldquo;kill switch&rdquo; for critical surfaces should:",
      "options": [
        "Depend on long cache TTLs to avoid flapping",
        "Require code redeploys to take effect",
        "Be limited to staging environments",
        "Be instantly actionable via synchronous control or fast&ndash;refresh rules"
      ],
      "correctAnswer": 3,
      "additionalContext": "High&ndash;risk flags need a rapid disable path: synchronous control plane or cached rules with short TTLs.",
      "keyConcepts": [
        "Kill switch",
        "TTL",
        "Critical paths"
      ]
    },
    {
      "id": "5",
      "type": "multiple-choice",
      "points": 2,
      "question": "What is &ldquo;flag debt&rdquo; and how do you avoid it?",
      "options": [
        "Latent complexity from stale flags; schedule removal sprints and automate cleanup",
        "A list of disabled flags to never remove",
        "The number of flags flipped per day; increase to reduce debt",
        "Debt measured only by experiment variants"
      ],
      "correctAnswer": 0,
      "additionalContext": "Stale flags add branching complexity and outage risk. Track intent and expiry; remove promptly after success.",
      "keyConcepts": [
        "Flag debt",
        "Cleanup",
        "Lifecycle"
      ]
    },
    {
      "id": "6",
      "type": "multiple-choice",
      "points": 2,
      "question": "Why log flag evaluations with subject/context keys?",
      "options": [
        "To eliminate the need for testing",
        "To enable audits and investigations of &ldquo;who saw what and why&rdquo;",
        "To speed up database queries",
        "To randomly sample user behavior"
      ],
      "correctAnswer": 1,
      "additionalContext": "Evaluation logs provide auditability and help explain outcomes in incidents or experiments.",
      "keyConcepts": [
        "Auditability",
        "Evaluation logs",
        "Compliance"
      ]
    },
    {
      "id": "7",
      "type": "multiple-choice",
      "points": 2,
      "question": "Governance guidance for sensitive flags (privacy, billing)?",
      "options": [
        "Single&ndash;operator control",
        "Public voting",
        "Dual control, approvals, and immutable logs",
        "Flip only during off&ndash;hours without records"
      ],
      "correctAnswer": 2,
      "additionalContext": "Sensitive flags should require stronger controls: approvals, dual control, and complete audit trails.",
      "keyConcepts": [
        "Governance",
        "Approvals",
        "Audit trail"
      ]
    },
    {
      "id": "8",
      "type": "multiple-choice",
      "points": 2,
      "question": "When should teams coordinate a change window for flips?",
      "options": [
        "For small internal tests only",
        "Never; flips should be silent",
        "Only after an incident has occurred",
        "For large audience changes where support/comms need preparation"
      ],
      "correctAnswer": 3,
      "additionalContext": "Coordinate windows for high&ndash;impact flips so support and stakeholders are prepared.",
      "keyConcepts": [
        "Change windows",
        "Stakeholder comms",
        "Support prep"
      ]
    },
    {
      "id": "9",
      "type": "freeform",
      "points": 4,
      "question": "Outline a flag lifecycle from creation to cleanup. Include owner, intent, telemetry thresholds for success, rollout plan, and a &ldquo;remove by&rdquo; date.",
      "sampleStrongResponse": "Create the flag with intent (&ldquo;release&rdquo;/&ldquo;ops&rdquo;/&ldquo;experiment&rdquo;), named owner, and remove&ndash;by date. Start with staff exposure, then ramp by percentage/cohort while tracking conversion, error rate, and p95 latency against thresholds. Maintain evaluation logs with subject/context. After success, execute a removal PR and delete targeting rules to avoid flag debt."
    },
    {
      "id": "10",
      "type": "freeform",
      "points": 5,
      "question": "Design a safe rollout for a high&ndash;risk payment feature behind a flag. Describe targeting, blast radius containment, kill switch behavior, and cleanup criteria.",
      "sampleStrongResponse": "Start internal&ndash;only, then 1% of a low&ndash;risk cohort by region, ramping while SLOs hold. Enable a global emergency off switch with short TTL rule refresh. Log all evaluations with user and reason. Predefine rollback triggers (error rate, p95 latency) and halt expansion if breached. Declare success criteria (conversion, error budgets) and schedule a removal PR once thresholds are met."
    }
  ]
},
  "integrated-development-environments": {
  "title": "Integrated Development Environments Knowledge Quiz",
  "totalQuestions": 10,
  "totalPoints": 25,
  "questions": [
    {
      "id": "1",
      "type": "multiple-choice",
      "points": 2,
      "question": "What does an IDE&rsquo;s code intelligence typically provide beyond basic syntax highlighting?",
      "options": [
        "Only color themes and font rendering",
        "Type‑aware completions, symbol navigation, and inline documentation",
        "A built‑in production deployment pipeline",
        "Automatic database schema migrations"
      ],
      "correctAnswer": 1,
      "additionalContext": "Modern IDE code intelligence uses parse trees, symbol indexes, and language servers to surface context‑aware suggestions, jump‑to‑definition, and inline docs, reducing cognitive load and navigation time.",
      "keyConcepts": [
        "Code intelligence",
        "Language Server Protocol",
        "Productivity"
      ]
    },
    {
      "id": "2",
      "type": "multiple-choice",
      "points": 2,
      "question": "What is a conditional breakpoint used for when debugging in an IDE?",
      "options": [
        "To always pause on every iteration",
        "To print logs without pausing execution",
        "To pause only when a specified expression evaluates to true",
        "To speed up execution by skipping lines"
      ],
      "correctAnswer": 2,
      "additionalContext": "Conditional breakpoints help isolate issues that occur only under certain states (for example, when a counter exceeds a threshold), avoiding noisy pauses and enabling focused inspection.",
      "keyConcepts": [
        "Debugger",
        "Breakpoints",
        "State inspection"
      ]
    },
    {
      "id": "3",
      "type": "multiple-choice",
      "points": 2,
      "question": "How do IDE profilers help address performance problems?",
      "options": [
        "They enforce code style rules",
        "They provision staging environments",
        "They manage Git branches automatically",
        "They measure CPU time, memory usage, and hot paths to find bottlenecks"
      ],
      "correctAnswer": 3,
      "additionalContext": "Profilers reveal where time and memory are actually spent (hot functions, allocations, blocking calls), allowing targeted optimizations instead of guesswork.",
      "keyConcepts": [
        "Profiling",
        "CPU hotspots",
        "Memory diagnostics"
      ]
    },
    {
      "id": "4",
      "type": "multiple-choice",
      "points": 2,
      "question": "What benefit does IDE test runner integration primarily provide?",
      "options": [
        "Run tests with focused filters, watch mode, and inline failure details",
        "Start and stop the production database",
        "Manage Kubernetes cluster scaling",
        "Generate UI wireframes automatically"
      ],
      "correctAnswer": 0,
      "additionalContext": "Tight test runner integration shortens feedback loops with quick filtering (by file, tag, or failed tests), watch re‑runs, and clickable stack traces.",
      "keyConcepts": [
        "Test runners",
        "Feedback loop",
        "Developer velocity"
      ]
    },
    {
      "id": "5",
      "type": "multiple-choice",
      "points": 2,
      "question": "What is the purpose of IDE refactor tools like Rename Symbol or Extract Function?",
      "options": [
        "To obfuscate variable names for security",
        "To randomly reorder code blocks to test resilience",
        "To safely update usages across the project while preserving behavior",
        "To minify code for production"
      ],
      "correctAnswer": 2,
      "additionalContext": "Refactor tools operate on the symbol graph, updating all references consistently (for example, renaming across files) and reducing manual, error‑prone edits.",
      "keyConcepts": [
        "Refactoring",
        "Symbol graph",
        "Safety"
      ]
    },
    {
      "id": "6",
      "type": "multiple-choice",
      "points": 2,
      "question": "In an IDE, what does the project model (workspace indexing) enable?",
      "options": [
        "Auto‑scaling of cloud infrastructure",
        "Live production feature flags",
        "Automated legal compliance reports",
        "Cross‑file navigation, find‑usages, and refactor accuracy"
      ],
      "correctAnswer": 3,
      "additionalContext": "Indexing builds a searchable map of symbols and relationships, powering accurate navigation (find usages) and safe refactors across large codebases.",
      "keyConcepts": [
        "Project model",
        "Indexing",
        "Find usages"
      ]
    },
    {
      "id": "7",
      "type": "multiple-choice",
      "points": 2,
      "question": "How does the Language Server Protocol (LSP) relate to IDE code intelligence?",
      "options": [
        "LSP is a UI theme engine",
        "LSP standardizes how editors/IDEs request symbols, diagnostics, and completions from language servers",
        "LSP is a Git hosting protocol",
        "LSP is only for building container images"
      ],
      "correctAnswer": 1,
      "additionalContext": "LSP decouples language smarts from the editor UI. A language server provides completions, diagnostics, and definitions to any LSP‑compatible client.",
      "keyConcepts": [
        "LSP",
        "Diagnostics",
        "Completions"
      ]
    },
    {
      "id": "8",
      "type": "multiple-choice",
      "points": 2,
      "question": "When is attaching an IDE debugger preferable to adding temporary print statements?",
      "options": [
        "When you need to inspect complex, stateful interactions and step through code paths",
        "When you only need final program output",
        "When the code runs once and cannot be paused",
        "When you are formatting code for readability only"
      ],
      "correctAnswer": 0,
      "additionalContext": "Debuggers provide granular control (step‑in, step‑over, watch expressions) to analyze state transitions that are hard to capture with ad‑hoc logs.",
      "keyConcepts": [
        "Debugger vs logs",
        "State analysis",
        "Step control"
      ]
    },
    {
      "id": "9",
      "type": "freeform",
      "points": 4,
      "question": "Describe a practical workflow to diagnose a slow test suite using IDE tools.",
      "sampleStrongResponse": "Start by scoping with the IDE&rsquo;s test explorer (filter to the slowest suites by duration), then run with watch to reproduce locally. Use the built‑in profiler to capture a CPU timeline and identify hot paths (for example, expensive setup/teardown or I/O). Inspect flame graphs for repeated allocations. Add conditional breakpoints around suspected hotspots to examine state without spamming logs. Finally, verify improvements by rerunning the filtered tests and reviewing before/after timings inside the IDE.",
      "keyConcepts": [
        "Test explorer",
        "Profiling",
        "Hot path isolation"
      ]
    },
    {
      "id": "10",
      "type": "freeform",
      "points": 5,
      "question": "Outline a safe refactor using IDE tools to extract a large code block into a function and verify correctness.",
      "sampleStrongResponse": "Use the IDE&rsquo;s Extract Function to move the block behind a clear signature with typed parameters and return value. Run &ldquo;find usages&rdquo; to confirm call sites and use Rename Symbol to align names. Execute unit tests via the IDE&rsquo;s runner and add a focused test if coverage is thin. If behavior is complex, attach the debugger and step through both the original and refactored paths with breakpoints to confirm identical state transitions.",
      "keyConcepts": [
        "Extract Function",
        "Rename Symbol",
        "Verification"
      ]
    }
  ]
},
  "lean-startup": {
  "title": "Lean Startup Knowledge Quiz",
  "totalQuestions": 10,
  "totalPoints": 24,
  "questions": [
    {
      "id": "1",
      "type": "multiple-choice",
      "points": 2,
      "question": "Core loop in Lean Startup:",
      "options": [
        "Build-Measure-Learn",
        "Plan-Execute-Control",
        "Design-Implement-Test",
        "Ideate-Scale-Exit"
      ],
      "correctAnswer": 0,
      "additionalContext": "Build experiments, measure results, and learn to refine the next iteration.",
      "keyConcepts": [
        "Experimentation",
        "Feedback Loop"
      ]
    },
    {
      "id": "2",
      "type": "multiple-choice",
      "points": 2,
      "question": "MVP stands for:",
      "options": [
        "Most Valuable Plan",
        "Minimum Viable Product",
        "Managed Vendor Product",
        "Market Validation Process"
      ],
      "correctAnswer": 1,
      "additionalContext": "MVP validates riskiest assumptions quickly with minimal investment.",
      "keyConcepts": [
        "MVP",
        "Validation"
      ]
    },
    {
      "id": "3",
      "type": "multiple-choice",
      "points": 2,
      "question": "Primary goal of MVP:",
      "options": [
        "Perfect UI",
        "Max features",
        "Validate assumptions quickly",
        "Full automation"
      ],
      "correctAnswer": 2,
      "additionalContext": "Focus on learning, not completeness; ship the smallest experiment that yields signal.",
      "keyConcepts": [
        "Assumptions",
        "Learning"
      ]
    },
    {
      "id": "4",
      "type": "multiple-choice",
      "points": 2,
      "question": "Actionable metrics are:",
      "options": [
        "Vanity",
        "Non‑repeatable",
        "Guide decisions",
        "Purely aesthetic"
      ],
      "correctAnswer": 2,
      "additionalContext": "They map directly to decisions (e.g., pricing change) and can be repeated.",
      "keyConcepts": [
        "Actionable Metrics",
        "Decision Making"
      ]
    },
    {
      "id": "5",
      "type": "multiple-choice",
      "points": 2,
      "question": "Pivot means:",
      "options": [
        "Stop all work",
        "Minor tweak",
        "Strategic shift based on learning",
        "Ignore data"
      ],
      "correctAnswer": 2,
      "additionalContext": "A pivot changes strategy while preserving the vision, based on validated learning.",
      "keyConcepts": [
        "Pivot",
        "Validated Learning"
      ]
    },
    {
      "id": "6",
      "type": "multiple-choice",
      "points": 2,
      "question": "Cohort analysis helps with:",
      "options": [
        "UI theming",
        "Deployment scheduling",
        "Understanding user behavior over time",
        "Randomization"
      ],
      "correctAnswer": 2,
      "additionalContext": "Tracks behavior by user cohorts to see retention and engagement patterns.",
      "keyConcepts": [
        "Cohorts",
        "Retention"
      ]
    },
    {
      "id": "7",
      "type": "multiple-choice",
      "points": 2,
      "question": "Riskiest assumption first principle:",
      "options": [
        "Build big first",
        "Test least risky",
        "Test most uncertain first",
        "Skip validation"
      ],
      "correctAnswer": 2,
      "additionalContext": "Test the assumption that would most undermine the idea if false.",
      "keyConcepts": [
        "Risk",
        "Prioritization"
      ]
    },
    {
      "id": "8",
      "type": "multiple-choice",
      "points": 2,
      "question": "Good MVP characteristic:",
      "options": [
        "Pixel‑perfect",
        "Quick to build and measure",
        "Large scope",
        "No instrumentation"
      ],
      "correctAnswer": 1,
      "additionalContext": "An MVP is quick to build and instrumented to measure outcomes.",
      "keyConcepts": [
        "Speed",
        "Instrumentation"
      ]
    },
    {
      "id": "9",
      "type": "freeform",
      "points": 4,
      "question": "Describe one experiment you would run to validate a pricing hypothesis and your success metric.",
      "sampleStrongResponse": "A/B test two price points on a landing page; measure conversion to signup and downstream retention at 2 weeks."
    },
    {
      "id": "10",
      "type": "freeform",
      "points": 4,
      "question": "Explain how you would decide to pivot vs persevere after two MVP iterations.",
      "sampleStrongResponse": "Compare actionable metrics to thresholds; if north‑star metric is below target with negative trend and qualitative feedback indicates mismatch, pivot with a new hypothesis."
    }
  ]
},
  "mob-programming": {
  "title": "Mob Programming Knowledge Quiz",
  "totalQuestions": 10,
  "totalPoints": 25,
  "questions": [
    {
      "id": "1",
      "type": "multiple-choice",
      "points": 2,
      "question": "Mob programming involves:",
      "options": [
        "The whole team collaborating at one keyboard with explicit roles and timeboxes",
        "Two developers working asynchronously",
        "Managers writing code with the team",
        "Solo development with periodic check‑ins"
      ],
      "correctAnswer": 0,
      "additionalContext": "Mob sessions synchronize attention and decision‑making via facilitation, roles, and timers.",
      "keyConcepts": [
        "Whole‑team",
        "Facilitation",
        "Timeboxes"
      ]
    },
    {
      "id": "2",
      "type": "multiple-choice",
      "points": 2,
      "question": "Key roles often used in mobs:",
      "options": [
        "Driver, Facilitator, Navigator(s)",
        "Scrum Master, Product Owner, Executive",
        "Only Driver and no other roles",
        "Auditor and Scribe only"
      ],
      "correctAnswer": 0,
      "additionalContext": "Driver operates the keyboard, Facilitator manages flow, Navigator(s) guide approach and architecture.",
      "keyConcepts": [
        "Driver",
        "Facilitator",
        "Navigator"
      ]
    },
    {
      "id": "3",
      "type": "multiple-choice",
      "points": 2,
      "question": "When mobs are most beneficial:",
      "options": [
        "Architecture definition and cross‑cutting decisions",
        "Routine formatting changes",
        "Late‑night hotfixing without context",
        "Simple CSS tweaks"
      ],
      "correctAnswer": 0,
      "additionalContext": "Mobs accelerate convergence on complex, high‑impact decisions and gnarly bugs.",
      "keyConcepts": [
        "Architecture",
        "Cross‑cutting decisions",
        "Gnarly bugs"
      ]
    },
    {
      "id": "4",
      "type": "multiple-choice",
      "points": 2,
      "question": "An expected outcome of effective mobbing:",
      "options": [
        "Reduced knowledge spread",
        "Wider knowledge spread and shared mental models",
        "Slower decisions on complex topics",
        "More rework later"
      ],
      "correctAnswer": 1,
      "additionalContext": "Collective validation reduces rework, and shared mental models increase throughput later.",
      "keyConcepts": [
        "Knowledge spread",
        "Shared models",
        "Reduced rework"
      ]
    },
    {
      "id": "5",
      "type": "multiple-choice",
      "points": 2,
      "question": "Risk to watch for in mobs and mitigation:",
      "options": [
        "Social loafing; rotate roles and keep contributions visible",
        "Excessive documentation; delete all notes",
        "Too many keyboards; add more keyboards",
        "No agenda; add more participants"
      ],
      "correctAnswer": 0,
      "additionalContext": "Timeboxed rotations and explicit contributions help maintain engagement.",
      "keyConcepts": [
        "Social loafing",
        "Rotation",
        "Engagement"
      ]
    },
    {
      "id": "6",
      "type": "multiple-choice",
      "points": 2,
      "question": "Anti‑pattern indicating a meeting smell:",
      "options": [
        "Defined agenda, outcomes, and breakout plan",
        "Undefined scope and no timeboxes",
        "Clear decision record (ADR) at the end",
        "Explicit handoffs to pairs for implementation"
      ],
      "correctAnswer": 1,
      "additionalContext": "Without agenda and timeboxes, mobs devolve into meetings. Create outcomes and breakouts.",
      "keyConcepts": [
        "Agenda",
        "Timeboxes",
        "Breakouts"
      ]
    },
    {
      "id": "7",
      "type": "multiple-choice",
      "points": 2,
      "question": "Artifact that helps post‑mob alignment:",
      "options": [
        "Unwritten verbal agreements",
        "Random chat logs",
        "Architecture Decision Records (ADRs) summarizing key decisions",
        "Private notes only"
      ],
      "correctAnswer": 2,
      "additionalContext": "Summarize key decisions and rationale into ADRs and issue descriptions for clear handoffs.",
      "keyConcepts": [
        "ADRs",
        "Handoffs",
        "Summaries"
      ]
    },
    {
      "id": "8",
      "type": "multiple-choice",
      "points": 2,
      "question": "Good follow‑up after a mob session:",
      "options": [
        "Rotate prompts across team members to sustain engagement",
        "Forget to record decisions",
        "Delay all actions until next quarter",
        "Discard the timer and cadence"
      ],
      "correctAnswer": 0,
      "additionalContext": "Rotate prompts and actions to keep engagement high and spread ownership.",
      "keyConcepts": [
        "Rotation",
        "Engagement",
        "Ownership"
      ]
    },
    {
      "id": "9",
      "type": "freeform",
      "points": 4,
      "question": "Design a 60‑minute mob for diagnosing a gnarly production bug. Include roles, agenda, timeboxes, and breakout triggers. How will you capture decisions?",
      "sampleStrongResponse": "Agenda: 5m context, 20m hypothesis generation, 20m focused investigation, 10m decision, 5m next steps. Roles: Facilitator, Driver, 1–2 Navigators. Breakout when two viable paths emerge. Capture decisions and rationale in an ADR and issue with owners and due dates."
    },
    {
      "id": "10",
      "type": "freeform",
      "points": 5,
      "question": "You have to define cross‑cutting architecture for a new platform. Propose a mob plan and success metrics to justify the time investment.",
      "sampleStrongResponse": "Plan: 90‑minute mob with rotating Driver/Navigators, explicit agenda, and timeboxed spikes; follow with ADRs and pair breakouts to implement. Success: fewer rework cycles, faster convergence on decisions, improved alignment scores, and reduced incident rate due to early validation."
    }
  ]
},
  "pair-programming": {
  "title": "Pair Programming Knowledge Quiz",
  "totalQuestions": 10,
  "totalPoints": 25,
  "questions": [
    {
      "id": "1",
      "type": "multiple-choice",
      "points": 2,
      "question": "Primary roles in pairing:",
      "options": [
        "Driver writes code; Navigator reviews in real time and thinks ahead",
        "Driver reviews while Navigator types",
        "Both type simultaneously on separate branches",
        "No defined roles; ad‑hoc collaboration only"
      ],
      "correctAnswer": 0,
      "additionalContext": "Clear roles improve focus: Driver handles the keyboard; Navigator scans for edge cases, design, and risks.",
      "keyConcepts": [
        "Driver",
        "Navigator",
        "Role clarity"
      ]
    },
    {
      "id": "2",
      "type": "multiple-choice",
      "points": 2,
      "question": "Recommended rotation cadence:",
      "options": [
        "Once per day",
        "Every 15–30 minutes",
        "Once per sprint",
        "No rotation necessary"
      ],
      "correctAnswer": 1,
      "additionalContext": "Regular rotation balances attention, maintains energy, and spreads knowledge evenly.",
      "keyConcepts": [
        "Rotation",
        "Cadence",
        "Attention management"
      ]
    },
    {
      "id": "3",
      "type": "multiple-choice",
      "points": 2,
      "question": "When pairing delivers the most value:",
      "options": [
        "Routine, low‑risk refactors",
        "Formatting‑only changes",
        "High‑risk or complex changes (security, performance, data)",
        "Late after release"
      ],
      "correctAnswer": 2,
      "additionalContext": "Use pairing for high‑risk or unknown problem spaces; solo is fine for small, low‑risk tasks.",
      "keyConcepts": [
        "Risk‑based pairing",
        "Complexity",
        "Exploration"
      ]
    },
    {
      "id": "4",
      "type": "multiple-choice",
      "points": 2,
      "question": "Team‑level outcome from consistent pairing:",
      "options": [
        "More single points of failure",
        "Longer onboarding time",
        "Fewer defects on trivial tasks",
        "Higher bus factor and fewer single points of failure"
      ],
      "correctAnswer": 3,
      "additionalContext": "Pairing spreads context, increasing the team&rsquo;s resilience to individual unavailability.",
      "keyConcepts": [
        "Bus factor",
        "Onboarding",
        "Quality"
      ]
    },
    {
      "id": "5",
      "type": "multiple-choice",
      "points": 2,
      "question": "A common trade‑off of pairing is:",
      "options": [
        "Better UI polish by default",
        "Guaranteed schedule acceleration",
        "Diminishing returns on well‑understood, low‑risk changes",
        "Elimination of all defects"
      ],
      "correctAnswer": 2,
      "additionalContext": "Schedule pairing during high‑value windows; avoid over‑pairing on simple, low‑risk work.",
      "keyConcepts": [
        "Trade‑offs",
        "Scheduling",
        "Value focus"
      ]
    },
    {
      "id": "6",
      "type": "multiple-choice",
      "points": 2,
      "question": "Remote ergonomics that matter most:",
      "options": [
        "Video always on with no exceptions",
        "Low‑latency tools with shared cursors, clear audio, and agreed handoffs",
        "Emailing code snippets between developers",
        "Screen sharing with 10‑second lag"
      ],
      "correctAnswer": 1,
      "additionalContext": "Optimize for low latency and clarity; video can be optional when prompts, checklists, and small commits are used.",
      "keyConcepts": [
        "Latency",
        "Shared cursors",
        "Handoffs"
      ]
    },
    {
      "id": "7",
      "type": "multiple-choice",
      "points": 2,
      "question": "Practice that improves remote pairing clarity:",
      "options": [
        "Use prompts, checklists, and small commits",
        "Disable commit messages to move faster",
        "Avoid writing tests until the end",
        "Hide the cursor to reduce distractions"
      ],
      "correctAnswer": 0,
      "additionalContext": "Shared context prompts and small, well‑described commits reduce confusion and rework.",
      "keyConcepts": [
        "Prompts",
        "Checklists",
        "Small commits"
      ]
    },
    {
      "id": "8",
      "type": "multiple-choice",
      "points": 2,
      "question": "Test‑first flow enabled by pairing:",
      "options": [
        "Skip tests to type faster",
        "Write tests only after deployment",
        "Defer acceptance criteria to a later sprint",
        "Scaffold unit tests and acceptance criteria while designing"
      ],
      "correctAnswer": 3,
      "additionalContext": "Pairing supports test‑first flows: clarify acceptance criteria and scaffold tests early.",
      "keyConcepts": [
        "Test‑first",
        "Acceptance criteria",
        "Scaffolding"
      ]
    },
    {
      "id": "9",
      "type": "freeform",
      "points": 4,
      "question": "Outline a pairing rotation policy for a new service (roles, 15–30 minute cadence, break conditions). Which metrics would you track to evaluate effectiveness?",
      "sampleStrongResponse": "Define Driver/Navigator rotation every 20 minutes with a timer and explicit handoff prompts. Break when latency, fatigue, or scope shifts require it. Track defect rates in complex paths, lead time for risky changes, onboarding time, and developer sentiment."
    },
    {
      "id": "10",
      "type": "freeform",
      "points": 5,
      "question": "You must land a risky concurrency change under time pressure. Propose a pairing plan (who, when, where) and justify the ROI. How will you measure success?",
      "sampleStrongResponse": "Pair a domain expert with an implementer in the highest‑risk code area during peak collaboration hours. Use shared cursors, prompts, and test‑first scaffolding. Success metrics: reduction in escaped defects, faster code review cycle time, stable performance metrics, and positive developer sentiment."
    }
  ]
},
  "risk-assessment": {
  "title": "Risk Assessment Knowledge Quiz",
  "totalQuestions": 10,
  "totalPoints": 25,
  "questions": [
    {
      "id": "1",
      "type": "multiple-choice",
      "points": 2,
      "question": "What is the primary purpose of a likelihood/impact matrix?",
      "options": [
        "To assign blame after incidents",
        "To create audit paperwork only",
        "To replace monitoring entirely",
        "To visualize risk exposure and prioritize mitigations"
      ],
      "correctAnswer": 3,
      "additionalContext": "The matrix highlights high likelihood/high impact risks for prioritization and mitigation planning.",
      "keyConcepts": [
        "Risk matrix",
        "Prioritization",
        "Exposure"
      ]
    },
    {
      "id": "2",
      "type": "multiple-choice",
      "points": 2,
      "question": "A mitigation differs from a contingency because:",
      "options": [
        "Mitigation is applied after the risk occurs; contingency is before",
        "They are identical",
        "Mitigation reduces likelihood/impact beforehand; contingency is the plan if the risk happens",
        "Contingency reduces probability only"
      ],
      "correctAnswer": 2,
      "additionalContext": "Mitigations act proactively to reduce probability or impact; contingencies are &ldquo;what we do if it happens&rdquo;.",
      "keyConcepts": [
        "Mitigation",
        "Contingency",
        "Proactive vs reactive"
      ]
    },
    {
      "id": "3",
      "type": "multiple-choice",
      "points": 2,
      "question": "A good trigger for a risk is:",
      "options": [
        "A specific observable event like error rate exceeding a threshold",
        "Vague concern with no observable signal",
        "A quarterly meeting invite",
        "A teammate&rsquo;s feeling"
      ],
      "correctAnswer": 0,
      "additionalContext": "Triggers should be measurable signals (metrics, logs, events) that indicate increased risk likelihood.",
      "keyConcepts": [
        "Triggers",
        "Signals",
        "Monitoring"
      ]
    },
    {
      "id": "4",
      "type": "multiple-choice",
      "points": 2,
      "question": "Which practice supports ongoing risk visibility during a project?",
      "options": [
        "Static risk list created once",
        "Risk burndown chart updated with discovery and mitigations",
        "Only postmortems",
        "Ignoring low probability risks"
      ],
      "correctAnswer": 1,
      "additionalContext": "A risk burndown tracks exposure over time and shows effect of mitigations and new discoveries.",
      "keyConcepts": [
        "Risk burndown",
        "Discovery",
        "Exposure over time"
      ]
    },
    {
      "id": "5",
      "type": "multiple-choice",
      "points": 2,
      "question": "When interpreting a risk burndown that is flat or rising:",
      "options": [
        "Everything is fine by default",
        "It proves schedule padding is too large",
        "It means we overestimated impact",
        "It indicates mitigations are insufficient or new risks emerged"
      ],
      "correctAnswer": 3,
      "additionalContext": "Flat or rising exposure suggests mitigations are ineffective or risks are being discovered faster than addressed.",
      "keyConcepts": [
        "Interpretation",
        "Exposure",
        "Mitigation effectiveness"
      ]
    },
    {
      "id": "6",
      "type": "multiple-choice",
      "points": 2,
      "question": "Scenario planning helps by:",
      "options": [
        "Eliminating all uncertainty",
        "Replacing incident response",
        "Defining responses for plausible futures and decision points",
        "Guaranteeing dates regardless of risk"
      ],
      "correctAnswer": 2,
      "additionalContext": "Scenarios outline plausible futures with triggers/decision points so teams can act quickly when signals occur.",
      "keyConcepts": [
        "Scenario planning",
        "Decision points",
        "Futures"
      ]
    },
    {
      "id": "7",
      "type": "multiple-choice",
      "points": 2,
      "question": "A common bias to avoid in risk assessment:",
      "options": [
        "Anchoring to first estimates without new data",
        "Using data from monitoring",
        "Considering multiple mitigation options",
        "Revisiting assumptions when signals change"
      ],
      "correctAnswer": 0,
      "additionalContext": "Anchoring and confirmation bias can distort likelihood/impact estimates; regularly revisit with fresh data.",
      "keyConcepts": [
        "Bias",
        "Anchoring",
        "Confirmation bias"
      ]
    },
    {
      "id": "8",
      "type": "multiple-choice",
      "points": 2,
      "question": "Which combination best reduces risk exposure before launch?",
      "options": [
        "Disable monitoring and ship",
        "Pilot with canary/feature flags plus rollback strategy",
        "Skip load testing to save time",
        "Ship on Friday evening"
      ],
      "correctAnswer": 1,
      "additionalContext": "Pilots with flags and clear rollback reduce blast radius and time to recover.",
      "keyConcepts": [
        "Pilot",
        "Feature flags",
        "Rollback"
      ]
    },
    {
      "id": "9",
      "type": "freeform",
      "points": 4,
      "question": "You inherit a project with high unknowns. Describe how you would establish a risk register with triggers, mitigations, and contingencies. How will you track exposure over time?",
      "sampleStrongResponse": "Create a lightweight register listing risk, likelihood, impact, owner, trigger, mitigation, contingency. Tie triggers to metrics/logs and review weekly. Track exposure via a risk burndown chart updated as mitigations land and new risks are discovered."
    },
    {
      "id": "10",
      "type": "freeform",
      "points": 5,
      "question": "An upcoming launch depends on an external API with uncertain limits. Propose mitigations and contingencies, including signals to switch strategies.",
      "sampleStrongResponse": "Mitigations: cache, rate limiting, backoff, prefetching, contract tests. Contingencies: failover path, toggle to reduced capability, staged rollout. Signals: elevated 429/5xx rate or latency p95 crossing threshold triggers fallback and rollback."
    }
  ]
},
  "rollback-procedures": {
  "title": "Rollback Procedures Knowledge Quiz",
  "totalQuestions": 10,
  "totalPoints": 25,
  "questions": [
    {
      "id": "1",
      "type": "multiple-choice",
      "points": 2,
      "question": "Core principle of reversible deployments?",
      "options": [
        "Design forward&ndash;only DB changes and idempotent operations with versioned artifacts",
        "Allow destructive DB changes under peak load",
        "Rely on manual recovery steps only",
        "Avoid testing rollbacks in staging"
      ],
      "correctAnswer": 0,
      "additionalContext": "Reversible deployments favor forward&ndash;only migrations, idempotent ops, immutable artifacts, and rehearsal of rollbacks.",
      "keyConcepts": [
        "Reversible deployments",
        "Idempotency",
        "Versioned artifacts"
      ]
    },
    {
      "id": "2",
      "type": "multiple-choice",
      "points": 2,
      "question": "Blue/Green vs canary &mdash; which mapping is correct?",
      "options": [
        "Blue/Green shifts a small percent first; canary flips all traffic instantly",
        "Blue/Green maintains two environments for atomic traffic switch; canary starts with a small percentage",
        "Both are identical strategies",
        "Canary requires DNS only; Blue/Green requires no routing changes"
      ],
      "correctAnswer": 1,
      "additionalContext": "Blue/Green runs two environments and flips traffic atomically for instant rollback; canary ramps a small percentage to detect regressions early.",
      "keyConcepts": [
        "Blue/Green",
        "Canary",
        "Rollback speed"
      ]
    },
    {
      "id": "3",
      "type": "multiple-choice",
      "points": 2,
      "question": "Observability requirement for safe rollback strategies?",
      "options": [
        "Informal logging only",
        "Manual dashboards after incidents",
        "SLO&ndash;linked signals (errors, p95 latency) with alerting",
        "Noisy alerts without thresholds"
      ],
      "correctAnswer": 2,
      "additionalContext": "Rollback triggers depend on timely signals tied to SLOs with alerting to detect regressions quickly.",
      "keyConcepts": [
        "Observability",
        "SLOs",
        "Alerts"
      ]
    },
    {
      "id": "4",
      "type": "multiple-choice",
      "points": 2,
      "question": "Database&ndash;safe change pattern during migrations?",
      "options": [
        "Destructive schema changes during peak",
        "Permanent dual&ndash;write architecture",
        "Skip backfill and reconcile later only if issues occur",
        "Forward&ndash;only additive changes; temporary dual&ndash;write/dual&ndash;read"
      ],
      "correctAnswer": 3,
      "additionalContext": "Prefer additive changes and short&ndash;lived dual&ndash;write/dual&ndash;read to migrate safely with clear backfill and reconciliation steps.",
      "keyConcepts": [
        "Forward&ndash;only",
        "Dual write/read",
        "Backfill"
      ]
    },
    {
      "id": "5",
      "type": "multiple-choice",
      "points": 2,
      "question": "Benefit of practicing rollbacks in staging?",
      "options": [
        "Reveals missing scripts and coupling before customers are impacted",
        "Increases surprise during production",
        "Eliminates the need for runbooks",
        "Guarantees zero incidents"
      ],
      "correctAnswer": 0,
      "additionalContext": "Rehearsals expose gaps in scripts and hidden coupling early, improving confidence and speed during incidents.",
      "keyConcepts": [
        "Rehearsal",
        "Staging",
        "Runbooks"
      ]
    },
    {
      "id": "6",
      "type": "multiple-choice",
      "points": 2,
      "question": "Automated rollback triggers should:",
      "options": [
        "Flip on any single noisy metric",
        "Use multi&ndash;signal confirmation tied to SLOs to avoid flapping",
        "Depend on manual approval only",
        "Ignore error budgets"
      ],
      "correctAnswer": 1,
      "additionalContext": "Combine metrics, logs, and synthetic checks with thresholds to trigger rollback reliably without flapping.",
      "keyConcepts": [
        "Triggers",
        "Multi&ndash;signal",
        "Error budgets"
      ]
    },
    {
      "id": "7",
      "type": "multiple-choice",
      "points": 2,
      "question": "Blast radius containment means:",
      "options": [
        "Roll out everywhere at once",
        "Disable monitoring to reduce noise",
        "Stagger rollouts by region/cohort and isolate experiments from core flows",
        "Dark launch UI to all users first"
      ],
      "correctAnswer": 2,
      "additionalContext": "Limit simultaneous risk by regional or cohort ramps, dark launches, and isolation of experimental changes.",
      "keyConcepts": [
        "Blast radius",
        "Staggered rollout",
        "Dark launch"
      ]
    },
    {
      "id": "8",
      "type": "multiple-choice",
      "points": 2,
      "question": "Why pin artifacts, infra, and migrations?",
      "options": [
        "To allow silent drift between environments",
        "To avoid tagging releases",
        "To remove the need for version control",
        "To reproduce prior states faithfully for deterministic rollback"
      ],
      "correctAnswer": 3,
      "additionalContext": "Pinning versions makes rollback deterministic by reproducing the previous known&ndash;good state.",
      "keyConcepts": [
        "Version pinning",
        "Determinism",
        "Immutability"
      ]
    },
    {
      "id": "9",
      "type": "freeform",
      "points": 4,
      "question": "Draft a per&ndash;service rollback checklist for a DB migration. Include health checks, version pinning, backfill steps, and owner acknowledgements.",
      "sampleStrongResponse": "Checklist: (1) Verify prior artifact and schema versions are available and pinned, (2) Pre&ndash;flight health checks green, (3) Backfill plan and verification queries prepared, (4) Dual&ndash;write toggle path and teardown plan documented, (5) Rollback triggers linked to SLO thresholds, (6) On&ndash;call and service owner acks recorded, (7) Post&ndash;rollback validation and changelog entry."
    },
    {
      "id": "10",
      "type": "freeform",
      "points": 5,
      "question": "Compare canary vs Blue/Green for a high&ndash;traffic service. When would you choose each, and what rollback signals/triggers would you configure?",
      "sampleStrongResponse": "Use canary for incremental risk: start at 1&ndash;5% to validate error rate and p95 latency with multi&ndash;signal confirmation; expand as signals stay green. Choose Blue/Green when you need instant rollback and minimal downtime: maintain two environments and flip traffic atomically. In both cases, define SLO thresholds, synthetic probes, and alerting; auto&ndash;revert on threshold breach and notify stakeholders with version and reason."
    }
  ]
},
  "sprint-planning": {
  "title": "Sprint Planning Knowledge Quiz",
  "totalQuestions": 10,
  "totalPoints": 25,
  "questions": [
    {
      "id": "1",
      "type": "multiple-choice",
      "points": 2,
      "question": "What best characterizes a strong sprint goal?",
      "options": [
        "A collection of unrelated tasks",
        "A committed list of every backlog item regardless of capacity",
        "A cohesive outcome that guides trade offs and sequencing",
        "An exact task list with hour estimates for each person"
      ],
      "correctAnswer": 2,
      "additionalContext": "A sprint goal describes a coherent outcome that focuses the team and enables trade offs in scope while preserving intent.",
      "keyConcepts": [
        "Sprint goal",
        "Focus",
        "Trade offs"
      ]
    },
    {
      "id": "2",
      "type": "multiple-choice",
      "points": 2,
      "question": "Preferred slicing approach for stories selected in planning:",
      "options": [
        "Horizontal slices by layer only",
        "Thin vertical slice that delivers end to end user value",
        "One large spike followed by a big bang delivery",
        "Only technical subtasks with no user impact"
      ],
      "correctAnswer": 1,
      "additionalContext": "Thin vertical slices validate value and integration early, reducing risk and carryover.",
      "keyConcepts": [
        "Slicing",
        "Vertical slice",
        "Risk reduction"
      ]
    },
    {
      "id": "3",
      "type": "multiple-choice",
      "points": 2,
      "question": "Capacity versus commitment guidance:",
      "options": [
        "Plan at 110 percent of capacity to push throughput",
        "Commit exactly to average velocity without buffer",
        "Ignore capacity if stakeholders need a date",
        "Plan below capacity and include buffer for unplanned and support"
      ],
      "correctAnswer": 3,
      "additionalContext": "Plan conservatively against capacity and historical velocity, reserving buffer for support and discovery.",
      "keyConcepts": [
        "Capacity",
        "Velocity",
        "Buffer"
      ]
    },
    {
      "id": "4",
      "type": "multiple-choice",
      "points": 2,
      "question": "How should carryover be handled at the end of a sprint?",
      "options": [
        "Split the work and keep only completed scope as done; roll the remainder",
        "Mark everything done if it is close",
        "Extend the sprint length to finish",
        "Count points for partially completed work"
      ],
      "correctAnswer": 0,
      "additionalContext": "Only work that meets the definition of done is counted; remaining scope is split or re planned.",
      "keyConcepts": [
        "Carryover",
        "Definition of done",
        "Splitting"
      ]
    },
    {
      "id": "5",
      "type": "multiple-choice",
      "points": 2,
      "question": "Definition of ready primarily ensures:",
      "options": [
        "Tasks are fully coded before planning",
        "Stories are clear, sized, and testable enough to plan",
        "Stakeholders have approved every detail",
        "Velocity will increase automatically"
      ],
      "correctAnswer": 1,
      "additionalContext": "Definition of ready sets entry quality so teams can plan and forecast with fewer surprises.",
      "keyConcepts": [
        "Definition of ready",
        "Planning quality"
      ]
    },
    {
      "id": "6",
      "type": "multiple-choice",
      "points": 2,
      "question": "Tasking during sprint planning is most useful when:",
      "options": [
        "It clarifies approach for complex stories without over specifying",
        "It specifies minute by minute actions for the sprint",
        "It replaces acceptance criteria",
        "It is skipped for all work to save time"
      ],
      "correctAnswer": 0,
      "additionalContext": "Lightweight tasking can expose risk and dependencies for complex items while avoiding premature detail.",
      "keyConcepts": [
        "Tasking",
        "Complexity",
        "Dependencies"
      ]
    },
    {
      "id": "7",
      "type": "multiple-choice",
      "points": 2,
      "question": "How should velocity be used in sprint planning?",
      "options": [
        "Set as a target to hit each sprint",
        "Compare teams and rank performance",
        "Ignore it and rely only on optimism",
        "Use historical team trend as an input to forecast"
      ],
      "correctAnswer": 3,
      "additionalContext": "Velocity is a team internal forecasting signal, not a target or comparison metric.",
      "keyConcepts": [
        "Velocity",
        "Forecasting",
        "Anti patterns"
      ]
    },
    {
      "id": "8",
      "type": "multiple-choice",
      "points": 2,
      "question": "A dependent item is not ready due to an external team. Best action during planning:",
      "options": [
        "Commit anyway and hope it resolves",
        "Overcommit to compensate",
        "Surface the risk, add mitigation or spike, and re sequence if possible",
        "Remove the story and cancel the sprint"
      ],
      "correctAnswer": 2,
      "additionalContext": "Identify dependencies early, plan spikes or mitigations, and choose items that keep the sprint goal achievable.",
      "keyConcepts": [
        "Dependencies",
        "Risk",
        "Spikes"
      ]
    },
    {
      "id": "9",
      "type": "freeform",
      "points": 4,
      "question": "Given three backlog items that touch the same user outcome, write a sprint goal and describe one vertical slice that proves value early.",
      "sampleStrongResponse": "Sprint goal focuses on a single outcome such as enabling self service password reset. Choose a thin slice that hits UI, API, and data path to complete one reset path, then iterate on edge cases."
    },
    {
      "id": "10",
      "type": "freeform",
      "points": 5,
      "question": "Your team averages a velocity range of 20 to 26 over the last six sprints. Support typically consumes two points. Describe your commitment and buffer for the next sprint and why.",
      "sampleStrongResponse": "Commit near the low end of historical range minus expected support, for example 18 to 20 points of planned work. Reserve explicit buffer for support and discovery. This preserves sprint goal focus and reduces rollover."
    }
  ]
},
  "technical-debt-management": {
  "title": "Technical Debt Management Knowledge Quiz",
  "totalQuestions": 10,
  "totalPoints": 25,
  "questions": [
    {
      "id": "1",
      "type": "multiple-choice",
      "points": 2,
      "question": "What is the “interest” on technical debt?",
      "options": [
        "Ongoing extra effort, defects, and slower delivery caused by the debt",
        "The one time refactor cost",
        "Financial expense from cloud invoices",
        "Depreciation of hardware"
      ],
      "correctAnswer": 0,
      "additionalContext": "Interest manifests as recurring costs: slower changes, more bugs, and reduced throughput until the debt is addressed.",
      "keyConcepts": [
        "Technical debt",
        "Interest",
        "Throughput"
      ]
    },
    {
      "id": "2",
      "type": "multiple-choice",
      "points": 2,
      "question": "Best first step to prioritize a portfolio of debt items:",
      "options": [
        "Sort alphabetically",
        "Estimate once and forget",
        "Only fix what engineers find annoying",
        "Create a simple impact versus effort matrix with triggers"
      ],
      "correctAnswer": 3,
      "additionalContext": "Use impact/effort and clear triggers (e.g., incident count, lead time) to decide when to pay down debt.",
      "keyConcepts": [
        "Prioritization",
        "Impact/Effort",
        "Triggers"
      ]
    },
    {
      "id": "3",
      "type": "multiple-choice",
      "points": 2,
      "question": "A safe refactor strategy for high risk areas includes:",
      "options": [
        "Large bang rewrite without tests",
        "Editing production directly",
        "Refactor behind feature flags with incremental steps and tests",
        "Skipping code review to move faster"
      ],
      "correctAnswer": 2,
      "additionalContext": "Incremental refactors with tests and flags reduce blast radius and allow progressive hardening.",
      "keyConcepts": [
        "Refactor",
        "Feature flags",
        "Incremental"
      ]
    },
    {
      "id": "4",
      "type": "multiple-choice",
      "points": 2,
      "question": "Which safety net most directly reduces refactor risk?",
      "options": [
        "Weekly email updates",
        "Automated tests in CI with fast feedback",
        "Extensive manual QA only",
        "Bigger PRs to keep context together"
      ],
      "correctAnswer": 1,
      "additionalContext": "Automated unit, integration, and contract tests in CI catch regressions quickly.",
      "keyConcepts": [
        "CI",
        "Automated tests",
        "Regression"
      ]
    },
    {
      "id": "5",
      "type": "multiple-choice",
      "points": 2,
      "question": "A clear guardrail for tech debt management is:",
      "options": [
        "No tests needed if code is simple",
        "Avoid writing ADRs for debt-related decisions",
        "Merge on red builds if change is urgent",
        "Block refactors that reduce coverage below threshold"
      ],
      "correctAnswer": 3,
      "additionalContext": "Policy-as-code guardrails keep quality bars intact during refactors.",
      "keyConcepts": [
        "Guardrails",
        "Policy as code",
        "Coverage"
      ]
    },
    {
      "id": "6",
      "type": "multiple-choice",
      "points": 2,
      "question": "Identifying technical debt effectively involves:",
      "options": [
        "Using signals like flaky tests, long lead time, hotspots, and incident history",
        "Only engineers logging annoyances",
        "Relying solely on intuition",
        "Ignoring production metrics"
      ],
      "correctAnswer": 0,
      "additionalContext": "Objective signals surface cost centers: hotspots in version control, MTTR, change failure rate, flaky tests.",
      "keyConcepts": [
        "Signals",
        "Hotspots",
        "Incidents"
      ]
    },
    {
      "id": "7",
      "type": "multiple-choice",
      "points": 2,
      "question": "When should you schedule debt work within sprints?",
      "options": [
        "Only when there is no feature work",
        "As dedicated slices within feature work or a small standing allocation",
        "Never; it fixes itself",
        "At the very end of a release only"
      ],
      "correctAnswer": 1,
      "additionalContext": "Blend debt work into the flow or reserve a small, explicit allocation to avoid perpetual deferral.",
      "keyConcepts": [
        "Scheduling",
        "Allocation",
        "Flow"
      ]
    },
    {
      "id": "8",
      "type": "multiple-choice",
      "points": 2,
      "question": "A telltale sign that a rewrite is riskier than an incremental refactor:",
      "options": [
        "Interfaces are stable and well tested",
        "There is robust observability and contract tests",
        "Large unknowns and undocumented behaviors exist",
        "The area has low coupling and clear boundaries"
      ],
      "correctAnswer": 2,
      "additionalContext": "Undocumented edge cases make big bang rewrites risky; prefer incremental changes with characterization tests.",
      "keyConcepts": [
        "Rewrite risk",
        "Characterization tests",
        "Unknowns"
      ]
    },
    {
      "id": "9",
      "type": "freeform",
      "points": 4,
      "question": "You have a payment service with frequent hotfixes due to brittle code. Outline a plan to stabilize it while paying down debt with safety nets.",
      "sampleStrongResponse": "Introduce contract and integration tests around critical paths, add logging and tracing, and refactor behind feature flags in small steps. Track change failure rate and MTTR; require green CI and coverage thresholds for merges."
    },
    {
      "id": "10",
      "type": "freeform",
      "points": 5,
      "question": "Propose a quarterly technical debt roadmap that balances feature delivery and debt. Include triggers to escalate specific items.",
      "sampleStrongResponse": "Reserve a 10–15% allocation for prioritized debt tied to measurable outcomes (lead time, CFR). Use an impact/effort matrix, set triggers like incident count or blocked PRs, and review monthly. Escalate items when triggers fire or variance grows."
    }
  ]
},
  "version-control-strategies": {
  "title": "Version Control Strategies Knowledge Quiz",
  "totalQuestions": 10,
  "totalPoints": 25,
  "questions": [
    {
      "id": "1",
      "type": "multiple-choice",
      "points": 2,
      "question": "Default strategy that keeps integration pain low for teams beyond 8&ndash;10 engineers?",
      "options": [
        "Trunk&ndash;based development with small, frequent merges and flags",
        "Long&ndash;lived GitFlow branches with quarterly integration",
        "Only release branches with extended code freezes",
        "Hotfixes directly on main without review"
      ],
      "correctAnswer": 0,
      "additionalContext": "Trunk&ndash;based development emphasizes short&ndash;lived branches and frequent merges to main, often behind flags, which reduces integration risk as teams scale.",
      "keyConcepts": [
        "Trunk&ndash;based",
        "Short&ndash;lived branches",
        "Feature flags"
      ]
    },
    {
      "id": "2",
      "type": "multiple-choice",
      "points": 2,
      "question": "Primary purpose of a release branch before shipping?",
      "options": [
        "Begin new feature work",
        "Rewrite history for readability",
        "Replace tagging and changelogs",
        "Stabilize a cut of main for hardening and targeted fixes"
      ],
      "correctAnswer": 3,
      "additionalContext": "Release branches capture a specific cut for final fixes, docs, and sign&ndash;off before shipping.",
      "keyConcepts": [
        "Release branches",
        "Stabilization",
        "Hardening"
      ]
    },
    {
      "id": "3",
      "type": "multiple-choice",
      "points": 2,
      "question": "Code freeze best practice during a stabilization window?",
      "options": [
        "Pause all changes indefinitely",
        "Continue all merges to main at full speed",
        "Pause risky changes; allow targeted fixes; keep freeze short",
        "Ban hotfixes across environments"
      ],
      "correctAnswer": 2,
      "additionalContext": "Code freezes should be short, minimize risk, and still allow targeted fixes that are backported to the release branch and forward&ndash;merged to main.",
      "keyConcepts": [
        "Code freeze",
        "Backporting",
        "Forward merge"
      ]
    },
    {
      "id": "4",
      "type": "multiple-choice",
      "points": 2,
      "question": "SemVer communicates impact as &ldquo;MAJOR.MINOR.PATCH&rdquo;. Which statement aligns with this?",
      "options": [
        "MAJOR adds features; MINOR breaks APIs; PATCH redesigns history",
        "MAJOR signals breaking changes; MINOR adds features; PATCH fixes bugs",
        "MAJOR/ MINOR/ PATCH are interchangeable labels",
        "PATCH is for experimental features behind flags only"
      ],
      "correctAnswer": 1,
      "additionalContext": "Semantic versioning communicates expected impact: MAJOR for breaking changes, MINOR for backward&ndash;compatible features, PATCH for fixes.",
      "keyConcepts": [
        "SemVer",
        "Tags",
        "Change visibility"
      ]
    },
    {
      "id": "5",
      "type": "multiple-choice",
      "points": 2,
      "question": "Safe standard for rebase vs merge?",
      "options": [
        "Rebase private branches; merge into shared branches to preserve integration context",
        "Rebase shared branches to keep history linear",
        "Always squash&ndash;merge into main to hide history",
        "Avoid merge commits entirely"
      ],
      "correctAnswer": 0,
      "additionalContext": "Rebase on private branches is safe; merging into shared branches preserves integration context and avoids rewriting public history.",
      "keyConcepts": [
        "Rebase",
        "Merge",
        "Shared history"
      ]
    },
    {
      "id": "6",
      "type": "multiple-choice",
      "points": 2,
      "question": "Risk of long&ndash;lived branches in GitFlow&ndash;style workflows?",
      "options": [
        "Reduced merge conflicts over time",
        "Simpler audits due to fewer commits",
        "Guaranteed faster lead time",
        "Drift and higher integration risk as divergence grows"
      ],
      "correctAnswer": 3,
      "additionalContext": "Long&ndash;lived branches drift from main, increasing merge conflicts and integration risk, especially with tightly coupled code.",
      "keyConcepts": [
        "Long&ndash;lived branches",
        "Merge conflicts",
        "Coupling"
      ]
    },
    {
      "id": "7",
      "type": "multiple-choice",
      "points": 2,
      "question": "Benefit of automating changelogs from commits or PR titles?",
      "options": [
        "Removes the need for tags",
        "Eliminates all release notes",
        "Improves change visibility and reduces manual error",
        "Allows arbitrary rewrite of history"
      ],
      "correctAnswer": 2,
      "additionalContext": "Automated changelogs increase transparency and reduce manual error, helping customers and internal teams understand &ldquo;what changed&rdquo;.",
      "keyConcepts": [
        "Changelogs",
        "Automation",
        "Visibility"
      ]
    },
    {
      "id": "8",
      "type": "multiple-choice",
      "points": 2,
      "question": "To avoid &ldquo;lost fix&rdquo; incidents when patching a release branch, the team should:",
      "options": [
        "Only patch the release branch",
        "Backport to the release branch and forward&ndash;merge to main",
        "Patch main only and hope the release picks it up",
        "Rebase main onto the release branch"
      ],
      "correctAnswer": 1,
      "additionalContext": "Patches applied to a release branch should also be forward&ndash;merged to main so fixes are preserved in future releases.",
      "keyConcepts": [
        "Backport",
        "Forward merge",
        "Release policy"
      ]
    },
    {
      "id": "9",
      "type": "freeform",
      "points": 4,
      "question": "Propose a release branch merge policy that prevents &ldquo;lost fix&rdquo; incidents. Include backport/forward&ndash;merge rules, tagging, and how changelogs are generated.",
      "sampleStrongResponse": "Use a stabilization branch per release. All critical fixes: (1) merge to release branch, (2) immediately forward&ndash;merge the same commit to main, (3) tag once sign&ndash;off passes. Automate changelog generation from PR titles that include SemVer intent (&ldquo;major&rdquo;/&ldquo;minor&rdquo;/&ldquo;patch&rdquo;). Protect shared branches; allow rebase only on private feature branches."
    },
    {
      "id": "10",
      "type": "freeform",
      "points": 5,
      "question": "Given a regulated program needing quarterly releases and audit trails, justify GitFlow vs trunk&ndash;based. Specify rebase/merge rules, code freeze handling, and tag/changelog automation.",
      "sampleStrongResponse": "Choose GitFlow for quarterly, audited releases: long&ndash;lived release branches for stabilization and formal sign&ndash;off; short code freezes limited to riskier changes. Rebase allowed only on private branches; merge to shared branches. Tag every release with SemVer and auto&ndash;generate changelogs from PR titles. Backport fixes to the release branch and forward&ndash;merge to main to avoid &ldquo;lost fix&rdquo; issues."
    }
  ]
}
};
