# Quiz Generation Framework for Learning App V2 (Updated: December 2024)

## 🔄 **Recent Improvements**

- Added quiz-article content alignment requirements
- Enhanced deployment readiness validation
- Strengthened answer position distribution tracking
- Added content synchronization guidelines
- Improved technical validation workflow

## 🎯 **Quiz Structure Requirements**

- **8 Multiple Choice questions** (2 points each = 16 points)
- **2 Freeform questions** (Q9: 4 points, Q10: 5 points = 9 points)
- **Total: 25 points per quiz**

## 🔗 **Quiz-Article Content Alignment (CRITICAL)**

### **Pre-Generation Requirements**

- [ ] **Review final article content** before generating quiz questions
- [ ] **Extract business metrics** from article MetricsCard components for quiz questions
- [ ] **Identify customer scenarios** and pain point quotes from article for quiz integration
- [ ] **Map technical concepts** covered in article to ensure quiz tests appropriate depth

### **Required Alignment Points**

1. **Business Metrics Synchronization**

   ```
   Article MetricsCard: "40-60% reduction in coordination overhead"
   ↓ MUST ALIGN WITH ↓
   Quiz Question: "Organizations see what reduction in cross-team coordination when bounded contexts align with team ownership?"
   Answer: "40-60%"
   ```

2. **Customer Pain Point Integration**

   ```
   Article Customer Scenario: "We're spending 70% of engineering time on service coordination instead of building features"
   ↓ MUST APPEAR IN ↓
   Quiz Question: Customer scenario options or freeform question requirements
   ```

3. **Technical Concept Coverage**
   ```
   Article Key Concepts: Dependency Inversion, Clean Boundaries, Layered Architecture
   ↓ MUST BE TESTED IN ↓
   Quiz Questions: Multiple choice and freeform questions covering these exact concepts
   ```

### **Content Synchronization Validation (ENFORCED)**

- [ ] All percentage metrics in quiz match article MetricsCard data exactly
- [ ] Customer quotes in quiz appear verbatim in article customer scenarios
- [ ] Technical concepts tested in quiz are explained in article content
- [ ] Company examples align between article and quiz (e.g., Netflix, Spotify case studies)

#### **Renderer Registry Cross-check (Pre-push)**

- [ ] For any article with new/updated quiz, confirm article ID is registered in `articleRenderers` (in `ArticleContentWrapper`).
- [ ] Navigate to `/article/[articleId]` and `/quiz/[articleId]` to confirm no placeholders and quiz loads to 25 points.

## ✅ **Multiple Choice Question Excellence**

### DO:

- Focus on **foundational concepts** with lasting business value
- Test understanding of **technical implications** and **decision frameworks**
- Explain challenges and provide **solution options** (not just recall)
- Include **specific language applications** and **business scenarios**
- Test knowledge useful for **strategic technology conversations**
- **RANDOMIZE correct answer positions** - avoid patterns like all answers being "B" [[memory:5286233]]
- Enforce **Anti‑POE option parity**: parallel grammar, similar length (±20% of longest), no unique qualifiers
- Write **plausible distractors** that fail on exactly one criterion (precondition, scope, trade‑off, causality)
- Keep options **semantically independent** (no option implies/contains another)
- Avoid absolute qualifiers (**always/never**) unless stating a definition

### DON'T:

- Ask trivia about **company names** or case study details
- Test random facts without foundational value
- Create questions where you can guess without understanding the concept
- Focus on memorizing specific metrics or dates
- **Place all correct answers in the same position** (e.g., always option B)
- Use **All of the above/None of the above**

### Proven Patterns:

### Anti‑POE Standards (ENFORCED)

- **Option shape parity**: same voice/tense; each option within ±20% of the longest.
- **Plausibility**: each distractor anchored to a real misconception; fails for one explicit reason only.
- **Independence**: no mutually exclusive pairs; no subset/superset relationships.
- **Qualifier hygiene**: avoid absolute language unless definitional; prefer "primarily/when/under".
- **Answer distribution**: near‑even A/B/C/D within each quiz.

### Distractor Taxonomy & Rubric

- Taxonomy: common misunderstanding; overgeneralization; missing precondition; inverted causality; trade‑off reversal.
- Author tags each distractor with one taxonomy label and a one‑sentence "why it's wrong".
- Rubric per MC (target ≥8/10): Plausibility (0–2), Specificity (0–2), Independence (0–2), Parity (0–2), Misconception anchoring (0–2).

```
✅ "Which factor makes compiled languages particularly suited for microservices architectures?"
✅ "A company experiencing 'Our cloud bills doubled but traffic only increased 20%' would benefit most from:"
✅ "What distinguishes object-oriented programming's approach to code organization?"
```

## 🎯 **Freeform Question Excellence**

### DO:

- Make questions **specific and prescriptive** (not open-ended)
- Include **clear success criteria** with measurable components
- Test **different foundational concepts** (avoid redundancy between Q9 and Q10)
- Focus on **essential information retention**
- Require **2-3 sentence minimum** responses
- Test knowledge useful for **understanding development team contexts**
- **Use listing format** - "Describe the three..." or "List the main..."

### TAM Lens (Technical Account Manager) Requirements

- Focus on **conversation readiness**: ask/align/verify language over implement details.
- Favor **guardrails and decision criteria**: transaction boundaries, freshness SLAs, phased migrations, slow‑query budgets, minimal indexes.
- Embed **Cursor leverage**: EXPLAIN in PRs, performance budgets, idempotent write patterns, rollback toggles.
- Stems should reflect **PR/design conversation prompts** rather than engine internals.

### Explanation (Mini‑Lesson) Requirements — TAM × Cursor

For each multiple‑choice question, the explanation must include:

- **Headline**: plain‑language promise/constraint
- **Why correct** and **why others are wrong** (1 line per distractor with taxonomy label)
- **Cursor leverage**: concrete AI task(s) to run in Cursor and expected artifacts (e.g., EXPLAIN summary, minimal index DDL, rollback script, SLA phrasing)
- **PR‑ready comment**: 1–2 sentences suitable to paste into a PR
- **Acceptance checks**: 2+ verifiable guardrails (e.g., slow‑query budget stated, freshness SLA documented, phased plan attached)

For freeform questions, include:

- **Scoring guidance** with measurable elements
- **Strong sample response** in TAM voice
- **Follow‑up actions** that reference Cursor artifacts

#### Structured Explanation Labeling (UI‑aware)

To render explanations as structured blocks in the app, prefix sections EXACTLY with these labels in `additionalContext`:

- `Headline:` one sentence
- `Why correct:` 1–2 sentences
- `Why others are wrong:` semicolon‑separated list (each item one short clause)
- `Cursor leverage:` semicolon‑separated list of concrete AI tasks/artifacts
- `Acceptance checks:` semicolon‑separated list of guardrails to verify

Notes:
- Use semicolons to create bullets; avoid commas for list separation.
- Keep the headline concise (no label text besides the value).

### DON'T:

- Create hypothetical TAM scenarios or business projections
- Ask about specific company examples or case studies
- Make questions too broad or philosophical
- Test knowledge that overlaps significantly with MC questions

### Proven Patterns:

```
✅ Q9 (4 pts): "Draft a PR comment that de‑risks a hot path: request EXPLAIN, state slow‑query budget, propose minimal index, and define rollback."

✅ Q10 (5 pts): "Outline a phased migration talk track: add → backfill → flip → enforce → clean up; include safety checks and comms plan."
```

## 🔧 **Content Focus Priorities**

1. **Language specializations** and technical strengths
2. **Technical requirements** that drive technology decisions
3. **Development environment** considerations
4. **Architectural implications** of language choices
5. **Performance characteristics** and trade-offs

### Coverage Map (REQUIRED before writing questions)

- Create a 6–10 item inventory of must‑retain concepts from the article (definitions, properties, constraints, trade‑offs, decision criteria, diagnosis signals).
- Map every MC to exactly one item; avoid trivia‑only questions.

Template:

```json
{
  "articleId": "...",
  "mustRetainConcepts": [
    "ACID: atomicity ensures all‑or‑nothing",
    "Isolation levels: RC vs RR vs Serializable",
    "Normalization to 3NF reduces anomalies",
    "FKs enforce referential integrity",
    "Index selectivity and write amplification"
  ]
}
```

### Numbers & Brand Usage

- Only use percentages/ranges that appear in article MetricsCards and have durable value; otherwise, avoid numbers.
- Test the abstraction behind company examples, not the proper noun.

## 💻 **Technical Implementation**

### JSON Structure:

```json
{
  "id": "question-id",
  "type": "multiple-choice" | "freeform",
  "question": "Clear, specific question text",
  "points": 2 | 4 | 5,
  "options": [...], // MC only
  "correctAnswer": "...", // MC only
  "explanation": "...", // MC only
  "sampleStrongResponse": "..." // Freeform only
}
```

### Scoring Logic:

- MC: `points: 2` for all questions
- Freeform Q9: `points: 4` (self-assessment: 4 excellent, 2 partial, 0 poor)
- Freeform Q10: `points: 5` (self-assessment: 5 excellent, 3 partial, 0 poor)

## 🚀 **Enhanced Deployment Validation**

### **Technical Validation**

- [ ] Quiz data displays correctly in development server (`npm run dev`)
- [ ] All question types render properly (multiple-choice, freeform, self-assessment)
- [ ] No TypeScript errors in quiz data structure
- [ ] No JSON syntax errors in learning-content.json
- [ ] Total points calculation works correctly (25 points)
- [ ] `npm run build` succeeds with quiz data included

### **Content-Article Alignment Validation**

- [ ] Business metrics in quiz questions match article MetricsCard components exactly
- [ ] Customer pain points quoted in quiz appear in article customer scenarios
- [ ] Technical concepts tested in quiz are covered in article content sections
- [ ] Company examples in quiz align with article case studies
- [ ] Freeform sample responses reflect article content depth and style

### **Answer Position Distribution Check**

- [ ] **Position A**: \_\_\_ questions (target: ~2)
- [ ] **Position B**: \_\_\_ questions (target: ~2)
- [ ] **Position C**: \_\_\_ questions (target: ~2)
- [ ] **Position D**: \_\_\_ questions (target: ~2)
- [ ] **No position bias detected** (avoid all answers being same letter)

### **Automated Anti‑POE Checks (Author or Script)**

- [ ] Option length variance within ±20% of longest option
- [ ] No "All of the above/None of the above"
- [ ] No absolute qualifiers unless definitional
- [ ] Options are parallel in grammar/voice
- [ ] Each distractor labeled with taxonomy + one‑sentence fail reason

## ✅ **Validation Checklist**

- [ ] 8 MC questions focusing on foundational concepts
- [ ] 2 Freeform questions with listing/specific format
- [ ] No redundancy between questions
- [ ] All questions test lasting knowledge value
- [ ] Clear success criteria for freeform responses
- [ ] Focus on languages/tech requirements (not company examples)
- [ ] **Correct answers distributed across positions A, B, C, D** (not all the same)
- [ ] **All quotes escaped** in question text and options (`&ldquo;`, `&rdquo;`, `&rsquo;`)
- [ ] Run `npm run build` to test before pushing
- [ ] Total points = 25

## 📌 **What Good Looks Like (POE Transformations)**

- Before: one hyper‑specific option vs three vague options → After: four parallel, concept‑specific options of similar length.
- Before: "Why denormalize?" with "speed" as the unique short answer → After: concrete workload patterns where duplication pays off; distractors fail on consistency or schema volatility.

## 📚 **Key Learnings from Compiled Languages Quiz**

- Questions about language specializations work better than company examples
- "List the core specialties of X, Y, Z languages" format is effective
- Avoid redundancy between MC performance questions and freeform architecture questions
- Pain point quotes from articles make good freeform question material
- Technical requirements should drive language selection discussions

## 🔄 **Complete Generation-to-Deployment Workflow**

### **Phase 1: Content Review (PRE-GENERATION)**

1. **Read final article content implementation** in ArticleContentWrapper.tsx
2. **Extract key business metrics** from MetricsCard components
3. **Identify customer scenarios** and quoted pain points
4. **Map core technical concepts** covered in article sections

### **Phase 2: Quiz Generation**

5. **Generate quiz questions** aligned with article content
6. **Verify answer position distribution** (aim for ~2 per position A,B,C,D)
7. **Check quote escaping** and JSON syntax throughout
8. **Ensure business metrics match** article data exactly

### **Phase 3: Integration Testing**

9. **Test quiz display** in development server (`npm run dev`)
10. **Verify TypeScript compilation** (`npm run build`)
11. **Check quiz component functionality** (all question types render)
12. **Validate total points calculation** (25 points total)

### **Phase 4: Content Alignment Validation**

13. **Cross-reference business metrics** between quiz and article MetricsCards
14. **Verify customer pain point quotes** appear in both quiz and article
15. **Ensure technical concept coverage alignment**
16. **Check company example consistency** (Netflix, Spotify, etc.)

### **Phase 5: Deployment Readiness**

17. **Final build verification** (`npm run build` succeeds)
18. **No TypeScript or linting errors** in modified files
19. **Local testing confirmation** (both article and quiz display correctly)
20. **Content-quiz synchronization confirmed** (no mismatched data)

## ✅ **Success Criteria**

A quiz implementation is complete when:

- ✅ All 8 MC + 2 freeform questions generated with proper alignment
- ✅ Business metrics in quiz match article MetricsCard data exactly
- ✅ Customer scenarios and pain points synchronized between quiz and article
- ✅ Technical concept coverage aligned with article content
- ✅ Answer positions distributed across A,B,C,D (no bias)
- ✅ TypeScript compilation succeeds and quiz displays properly
- ✅ Total 25 points and all question types function correctly

## ✳️ TAM × Cursor Quiz Update Prompt (Database Articles)

Use this comprehensive prompt to generate or refactor quizzes for database‑related articles (e.g., `relational-databases`, `document-databases`, `graph-databases`, `key-value-stores`, `normalization`, `indexing-strategies`, `query-optimization`, `schema-design`).

```text
ROLE: You are updating a quiz for the Learning App V2 as a Technical Account Manager coach. You write questions that prepare a TAM to lead PR/design conversations using Cursor, not to implement internals.

INPUTS
- articleId: <ARTICLE_ID>
- article file: src/components/articles/<ARTICLE_ID>.tsx
- goals: educate via quiz; avoid POE wins; use structured explanations; align to article sections

REQUIREMENTS
- Structure: 8 multiple‑choice (2 pts each) + 2 freeform (4 pts, 5 pts) = 25 points
- Anti‑POE: parity, plausibility, independence, no absolutes, no all/none; balanced answers A/B/C/D (~2 each)
- TAM lens: stems are PR/design conversation prompts that emphasize ask/align/verify/guardrails
- Coverage: map questions to Key Concepts, What good looks like, Failure signals, TL;DR
- Explanations: use labeled sections EXACTLY as below in `additionalContext`
  - Headline: <one sentence>
  - Why correct: <1–2 sentences>
  - Why others are wrong: item A; item B; item C (semicolon list)
  - Cursor leverage: task 1; task 2; task 3 (semicolon list)
  - Acceptance checks: check 1; check 2; check 3 (semicolon list)
- JSON safety: escape quotes (&ldquo; &rdquo; &rsquo;) where needed in text

CONTENT BLUEPRINT
- 3 Foundations (definitions/properties/constraints stated as promises/expectations)
- 3 Trade‑offs & Guardrails (replicas freshness windows, indexing write cost, isolation throughput, denormalization trust)
- 2 Diagnosis/Expectation (failure signals → likely cause → action)
- Freeform 1: PR comment that de‑risks a hot path (EXPLAIN, slow‑query budget, minimal index, rollback)
- Freeform 2: Phased migration talk track (add → backfill → flip → enforce → cleanup) with safety & comms

DELIVERABLE
- A JSON object for data/quizzes/<ARTICLE_ID>.json matching the app schema. For each MC `additionalContext`, use the labeled sections so the UI renders structured blocks.

CURSOR PROMPT SNIPPETS (embed into explanations as needed)
- EXPLAIN reviewer: Summarize plan; flag full scans; propose minimal composite index; estimate write amplification; generate rollback DDL
- Migration planner: Add → backfill (batches) → flip (flag) → enforce → cleanup; include idempotency, metrics, rollback toggle
- SLA framer: Draft replica freshness window language and primary/replica routing guidance; add simple lag monitoring

QUALITY GATES (author self‑check)
- Each MC maps to one must‑retain concept and includes a PR‑ready ask
- Explanations follow the labeled structure and include Cursor leverage + acceptance checks
- Options are parallel and plausible; no subset/superset; no absolutes
- Answer positions A/B/C/D balanced; quotes escaped; total points 25
```
